<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>2022Fall_人工智能笔记 | Keyspire&#39;s Peakory</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <meta name="description" content="人工智能笔记机器学习概览总结一下，机器学习善于：

需要进行大量手工调整或需要拥有长串规则才能解决的问题：机器学习算法通常可以简化代码、提高性能。
问题复杂，传统方法难以解决：最好的机器学习方法可以找到解决方案。
环境有波动：机器学习算法可以适应新数据
洞察复杂问题和大量数据。

监督学习和非监督学">
  
  
  
    <link rel="shortcut icon" href="../../../../../img/favicon.ico">
  
  <link rel="stylesheet" href="../../../../../css/style.css">
  
    <link rel="stylesheet" href="../../../../../fancybox/jquery.fancybox-1.3.4.css">
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <div id="nav-outer">
  <nav id="main-nav" class="outer">
    <a id="main-nav-toggle" class="nav-icon"></a>
    
      <a class="main-nav-link" href="../../../../../index.html">Home</a>
    
      <a class="main-nav-link" href="../../../../../way2/archives">Archives</a>
    
      <a class="main-nav-link" href="../../../../../about">About</a>
    
    <div class="main-nav-space-between"></div>
    
  </nav>
</div>
<div id="header-title">
  <h1 id="logo-wrap">
    <a href="../../../../../index.html" id="logo">Keyspire&#39;s Peakory</a>
  </h1>
  
</div>

      <div id="content" class="outer">
        <section id="main"><article id="post-notes/人工智能笔记" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="" class="article-date">
  <time class="dt-published" datetime="2023-02-20T16:00:00.000Z" itemprop="datePublished">2023-02-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      2022Fall_人工智能笔记
    </h1>
  

      </header>
    
    
<div id="article-toc">
    <h2 class="widget-title">目录</h2>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%AC%94%E8%AE%B0"><span class="toc-number">1.</span> <span class="toc-text">人工智能笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88"><span class="toc-number">1.1.</span> <span class="toc-text">机器学习概览</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%92%8C%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.1.1.</span> <span class="toc-text">监督学习和非监督学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.1.2.</span> <span class="toc-text">强化学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E5%92%8C%E5%9C%A8%E7%BA%BF%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.1.3.</span> <span class="toc-text">批量和在线学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.1.3.1.</span> <span class="toc-text">批量学习</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%A8%E7%BA%BF%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.1.3.2.</span> <span class="toc-text">在线学习</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">1.2.</span> <span class="toc-text">线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%B1%82%E8%A7%A3MSE"><span class="toc-number">1.2.1.</span> <span class="toc-text">正规方程求解MSE</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">1.3.</span> <span class="toc-text">如何训练模型?梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">1.3.1.</span> <span class="toc-text">批量梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">1.3.2.</span> <span class="toc-text">随机梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">1.3.3.</span> <span class="toc-text">小批量梯度下降</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF-x2F-%E8%BF%87%E6%8B%9F%E5%90%88-x2F-%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-number">1.4.</span> <span class="toc-text">学习曲线&#x2F;过拟合&#x2F;欠拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%AF%E5%B7%AE%E6%9D%A5%E6%BA%90%E5%88%86%E6%9E%90"><span class="toc-number">1.4.1.</span> <span class="toc-text">误差来源分析</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">1.5.</span> <span class="toc-text">正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B2%AD%E5%9B%9E%E5%BD%92"><span class="toc-number">1.5.1.</span> <span class="toc-text">岭回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Lasso%E5%9B%9E%E5%BD%92"><span class="toc-number">1.5.2.</span> <span class="toc-text">Lasso回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%B9%E6%80%A7%E7%BD%91%E7%BB%9C%EF%BC%88ElasticNet%EF%BC%89"><span class="toc-number">1.5.3.</span> <span class="toc-text">弹性网络（ElasticNet）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#EarlyStopping"><span class="toc-number">1.5.4.</span> <span class="toc-text">_EarlyStopping</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80"><span class="toc-number">1.6.</span> <span class="toc-text">机器学习基础</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM"><span class="toc-number">1.7.</span> <span class="toc-text">支持向量机SVM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7SVM%E5%88%86%E7%B1%BB"><span class="toc-number">1.7.1.</span> <span class="toc-text">线性SVM分类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BD%AF%E9%97%B4%E9%9A%94%E5%88%86%E7%B1%BB"><span class="toc-number">1.7.1.1.</span> <span class="toc-text">软间隔分类</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7SVM%E5%88%86%E7%B1%BB"><span class="toc-number">1.7.2.</span> <span class="toc-text">非线性SVM分类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%A0%B8"><span class="toc-number">1.7.2.1.</span> <span class="toc-text">多项式核</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A2%9E%E5%8A%A0%E7%9B%B8%E4%BC%BC%E7%89%B9%E5%BE%81"><span class="toc-number">1.7.2.2.</span> <span class="toc-text">增加相似特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%AB%98%E6%96%AF-RBF-%E6%A0%B8"><span class="toc-number">1.7.2.3.</span> <span class="toc-text">高斯 RBF 核</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-number">1.7.2.4.</span> <span class="toc-text">计算复杂度</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SVM%E5%9B%9E%E5%BD%92"><span class="toc-number">1.7.3.</span> <span class="toc-text">SVM回归</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">1.8.</span> <span class="toc-text">决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%B1%BB"><span class="toc-number">1.8.1.</span> <span class="toc-text">分类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#CART%E8%AE%AD%E7%BB%83%E7%AE%97%E6%B3%95"><span class="toc-number">1.8.1.1.</span> <span class="toc-text">_CART训练算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96-1"><span class="toc-number">1.8.1.2.</span> <span class="toc-text">正则化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92"><span class="toc-number">1.8.2.</span> <span class="toc-text">回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E7%A8%B3%E5%AE%9A%E6%80%A7"><span class="toc-number">1.8.3.</span> <span class="toc-text">不稳定性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-x2F-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">1.9.</span> <span class="toc-text">集成学习&#x2F;随机森林</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8A%95%E7%A5%A8-%E5%88%86%E7%B1%BB"><span class="toc-number">1.9.1.</span> <span class="toc-text">投票(分类)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bagging-amp-Pasting"><span class="toc-number">1.9.2.</span> <span class="toc-text">Bagging &amp; Pasting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">1.9.3.</span> <span class="toc-text">随机森林</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Boosting"><span class="toc-number">1.9.4.</span> <span class="toc-text">Boosting</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#AdaBoost"><span class="toc-number">1.9.4.1.</span> <span class="toc-text">AdaBoost</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87"><span class="toc-number">1.9.4.2.</span> <span class="toc-text">梯度提升</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stacking"><span class="toc-number">1.9.5.</span> <span class="toc-text">Stacking</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Maxout"><span class="toc-number">1.9.6.</span> <span class="toc-text">Maxout</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9B%B4%E5%BF%AB%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">1.10.</span> <span class="toc-text">更快的优化器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96Regularization"><span class="toc-number">1.11.</span> <span class="toc-text">正则化Regularization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Dropout"><span class="toc-number">1.11.1.</span> <span class="toc-text">Dropout</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN-CV"><span class="toc-number">1.12.</span> <span class="toc-text">CNN-CV</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82ConvalutionalLayer"><span class="toc-number">1.12.0.1.</span> <span class="toc-text">卷积层ConvalutionalLayer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-number">1.12.0.2.</span> <span class="toc-text">池化层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-number">1.12.0.3.</span> <span class="toc-text">数据增强</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#CNN%E7%9A%84%E5%85%B8%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-number">1.12.0.4.</span> <span class="toc-text">CNN的典型架构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B"><span class="toc-number">1.12.0.5.</span> <span class="toc-text">目标检测</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2"><span class="toc-number">1.12.0.6.</span> <span class="toc-text">语义分割</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RNN"><span class="toc-number">1.13.</span> <span class="toc-text">RNN</span></a></li></ol></li></ol>
</div>

    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="人工智能笔记"><a href="#人工智能笔记" class="headerlink" title="人工智能笔记"></a>人工智能笔记</h1><h2 id="机器学习概览"><a href="#机器学习概览" class="headerlink" title="机器学习概览"></a>机器学习概览</h2><p>总结一下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>机器学习善于<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<ul>
<li>需要进行大量手工调整或需要拥有长串规则才能解决的问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>机器学习算法通常可以简化代码<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>提高性能<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></li>
<li>问题复杂<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>传统方法难以解决<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>最好的机器学习方法可以找到解决方案<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></li>
<li>环境有波动<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>机器学习算法可以适应新数据</li>
<li>洞察复杂问题和大量数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></li>
</ul>
<h3 id="监督学习和非监督学习"><a href="#监督学习和非监督学习" class="headerlink" title="监督学习和非监督学习"></a>监督学习和非监督学习</h3><p>在监督学习中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>用来训练算法的训练数据包含了答案<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>称为标签<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>图 1-5<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/1-5.png"></p>
<p>图 1-5 用于监督学习<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>比如垃圾邮件分类<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>的加了标签的训练集</p>
<p>一个典型的监督学习任务是分类<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>垃圾邮件过滤器就是一个很好的例子<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>用许多带有归类<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>垃圾邮件或普通邮件<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>的邮件样本进行训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>过滤器必须还能对新邮件进行分类<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>另一个典型任务是预测目标数值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>例如给出一些特征<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>里程数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>车龄<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>品牌等等<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>称作预测值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>来预测一辆汽车的价格<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这类任务称作回归<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>图 1-6<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>要训练这个系统<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你需要给出大量汽车样本<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>包括它们的预测值和标签<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>即<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它们的价格<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>下面是一些重要的监督学习算法<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>本书都有介绍<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<ul>
<li>K 近邻算法</li>
<li>线性回归</li>
<li>逻辑回归</li>
<li>支持向量机<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>SVM<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></li>
<li>决策树和随机森林  </li>
<li>神经网络</li>
</ul>
<p>在非监督学习中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你可能猜到了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>训练数据是没有加标签的<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>图 1-7<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>系统在没有老师的条件下进行学习<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/1-7.png"></p>
<p>下面是一些最重要的非监督学习算法<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>我们会在第 8 章介绍降维<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<ul>
<li><strong>聚类</strong><br>K 均值<br>层次聚类分析<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>Hierarchical Cluster Analysis<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>HCA<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span><br>期望最大值</li>
<li><strong>可视化和降维</strong><br>主成分分析<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>Principal Component Analysis<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>PCA<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span><br>核主成分分析<br>局部线性嵌入<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>Locally-Linear Embedding<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>LLE<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span><br>t-分布邻域嵌入算法<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>t-distributed Stochastic Neighbor Embedding<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>t-SNE<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></li>
<li><strong>关联性规则学习</strong><br>Apriori 算法<br>Eclat 算法</li>
</ul>
<p>一些算法可以处理部分带标签的训练数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>通常是大量不带标签数据加上小部分带标签数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这称作<strong>半监督学习</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>图 1-11<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>一些图片存储服务<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>比如 Google Photos<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>是半监督学习的好例子<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>一旦你上传了所有家庭相片<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它就能自动识别到人物 A 出现在了相片 1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>5<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>11 中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>另一个人 B 出现在了相片 2<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>5<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>7 中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这是算法的非监督部分<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>聚类<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>现在系统需要的就是你告诉它这两个人是谁<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>只要给每个人一个标签<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>算法就可以命名每张照片中的每个人<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>特别适合搜索照片<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><p>强化学习非常不同<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>学习系统在这里被称为智能体<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>agent<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以对环境进行观察<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>选择和执行动作<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并获得奖励作为回报<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>负奖励是惩罚<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>见图 1-12<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>然后它必须自己学习哪个是最佳方法<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>称为策略<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>policy<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>以得到长久的最大奖励<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>策略决定了智能体在给定情况下应该采取的行动<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h3 id="批量和在线学习"><a href="#批量和在线学习" class="headerlink" title="批量和在线学习"></a>批量和在线学习</h3><h4 id="批量学习"><a href="#批量学习" class="headerlink" title="批量学习"></a>批量学习</h4><p>在批量学习中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>系统不能进行持续学习<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>必须用所有可用数据进行训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这通常会占用大量时间和计算资源<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以一般是线下做的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>首先是进行训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后部署在生产环境且停止学习<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它只是使用已经学到的策略<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这称为离线学习<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>如果你想让一个批量学习系统明白新数据<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>例如垃圾邮件的新类型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>就需要从头训练一个系统的新版本<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使用全部数据集<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>不仅有新数据也有老数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后停掉老系统<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>换上新系统<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>幸运的是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>评估<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>部署一套机器学习的系统的整个过程可以自动进行<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>见图 1-3<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以即便是批量学习也可以适应改变<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>只要有需要<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>就可以方便地更新数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>训练一个新版本<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>这个方法很简单<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>通常可以满足需求<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是用全部数据集进行训练会花费大量时间<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以一般是每 24 小时或每周训练一个新系统<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>如果系统需要快速适应变化的数据<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>比如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>预测股价变化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>就需要一个响应更及时的方案<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h4 id="在线学习"><a href="#在线学习" class="headerlink" title="在线学习"></a>在线学习</h4><p>在在线学习中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>是用数据实例持续地进行训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以一次一个或一次几个实例<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>称为小批量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>每个学习步骤都很快且廉价<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以系统可以动态地学习收到的最新数据<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>见图 1-13<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/1-13.png"></p>
<p>图 1-13 在线学习</p>
<p>在线学习很适合系统接收连续流的数据<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>比如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>股票价格<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>且需要自动对改变作出调整<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>如果计算资源有限<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在线学习是一个不错的方案<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>一旦在线学习系统学习了新的数据实例<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它就不再需要这些数据了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以扔掉这些数据<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>除非你想滚回到之前的一个状态<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>再次使用数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这样可以节省大量的空间<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>在线学习算法也适用于在超大数据集<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>一台计算机不足以用于存储它<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>上训练系统<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>这称作核外学习<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><em>out-of-core</em> learning<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>算法每次只加载部分数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>用这些数据进行训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后重复这个过程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>直到使用完所有数据<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>见图 1-14<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<blockquote>
<p>警告<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>这个整个过程通常是离线完成的<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>即<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不在部署的系统上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以在线学习这个名字会让人疑惑<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>可以把它想成持续学习<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</blockquote>
<p><img src="/img/loading.gif" data-original="/img/1-14.png"></p>
<p>图 1-14 使用在线学习处理大量数据集</p>
<p>在线学习系统的一个重要参数是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它们可以多快地适应数据的改变<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>这被称为学习速率<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>如果你设定一个高学习速率<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>系统就可以快速适应新数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是也会快速忘记老数据<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>你可不想让垃圾邮件过滤器只标记最新的垃圾邮件种类<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>相反的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果你设定的学习速率低<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>系统的惰性就会强<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>即<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它学的更慢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但对新数据中的噪声或没有代表性的数据点结果不那么敏感<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>在线学习的挑战之一是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果坏数据被用来进行训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>系统的性能就会逐渐下滑<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>如果这是一个部署的系统<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>用户就会注意到<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>坏数据可能来自失灵的传感器或机器人<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>或某人向搜索引擎传入垃圾信息以提高搜索排名<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>要减小这种风险<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你需要密集监测<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果检测到性能下降<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>要快速关闭<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>或是滚回到一个之前的状态<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>你可能还要监测输入数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>对反常数据做出反应<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>比如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使用异常检测算法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>在第一章<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们介绍了一个简单的生活满意度回归模型:</p>
<p><img src="/img/loading.gif" data-original="/img/tex-2b4fc5fcdceb2e12c666415e9ebb793a.gif"></p>
<p>这个模型仅仅是输入量<code>GDP_per_capita</code>的线性函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><code>θ[0]</code>和<code>θ[1]</code>是这个模型的参数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>线性模型更一般化的描述指通过计算输入变量的加权和<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并加上一个常数偏置项<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>截距项<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>来得到一个预测值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>如公式 4-1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p>公式 4-1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>线性回归预测模型</p>
<p><img src="/img/loading.gif" data-original="tex-f99876b625a13a0aad9631f61d934a61.gif"></p>
<ul>
<li><p><code>y_hat</code>表示预测结果</p>
</li>
<li><p><code>n</code>表示特征的个数</p>
</li>
<li><p><code>x[i]</code>表示第<code>i</code>个特征的值</p>
</li>
<li><p><code>θ[j]</code>表示第<code>j</code>个参数<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>包括偏置项<code>θ[0]</code>和特征权重值<code>θ[1], θ[2], ..., θ[nj]</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p>
</li>
</ul>
<p>上述公式可以写成更为简洁的向量形式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如公式 4-2<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p>公式 4-2<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>线性回归预测模型<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>向量形式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/tex-5da22015388cdeacf9c75c3511592953.gif"></p>
<ul>
<li><p><code>θ</code>表示模型的参数向量包括偏置项<code>θ[0]</code>和特征权重值<code>θ[1]</code>到<code>θ[n]</code></p>
</li>
<li><p><code>θ^T</code>表示向量<code>θ</code>的转置<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>行向量变为了列向量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p>
</li>
<li><p><code>x</code>为每个样本中特征值的向量形式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>包括<code>x[1]</code>到<code>x[n]</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而且<code>x[0]</code>恒为 1</p>
</li>
<li><p><code>θ^T · x</code>表示<code>θ^T</code>和<code>x</code>的点积</p>
</li>
<li><p><code>h[θ]</code>表示参数为<code>θ</code>的假设函数</p>
</li>
</ul>
<p>怎么样去训练一个线性回归模型呢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>？</h-inner></h-char></span>好吧<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>回想一下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>训练一个模型指的是设置模型的参数使得这个模型在训练集的表现较好<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>为此<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们首先需要找到一个衡量模型好坏的评定方法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>在第二章<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们介绍到在回归模型上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>最常见的评定标准是均方根误差<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>RMSE<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>详见公式 2-1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>因此<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>为了训练一个线性回归模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你需要找到一个<code>θ</code>值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它使得均方根误差<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>标准误差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>达到最小值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>实践过程中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>最小化均方误差比最小化均方根误差更加的简单<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这两个过程会得到相同的<code>θ</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因为函数在最小值时候的自变量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>同样能使函数的方根运算得到最小值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>在训练集<code>X</code>上使用公式 4-3 来计算线性回归假设<code>h[θ]</code>的均方差<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>MSE<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>公式 4-3<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>线性回归模型的 MSE 损失函数</p>
<p><img src="/img/loading.gif" data-original="/img/tex-e42dee8953b9b2be4a3ed6f8c09e5314.gif"></p>
<p>公式中符号的含义大多数都在第二章<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>详见<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>符号<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>进行了说明<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不同的是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>为了突出模型的参数向量<code>θ</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使用<code>h[θ]</code>来代替<code>h</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>以后的使用中为了公式的简洁<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使用<code>MSE(θ)</code>来代替<code>MSE(X, h[θ])</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h3 id="正规方程求解MSE"><a href="#正规方程求解MSE" class="headerlink" title="正规方程求解MSE"></a>正规方程求解MSE</h3><p>为了找到最小化损失函数的<code>θ</code>值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以采用公式解<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>换句话说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>就是可以通过解正规方程直接得到最后的结果<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>公式 4-4<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>正规方程</p>
<p><img src="/img/loading.gif" data-original="/img/tex-43bfb04cdbbd85ad21489e8e2dc853ed.gif"></p>
<ul>
<li><code>θ_hat</code>指最小化损失<code>θ</code>的值</li>
<li><code>y</code>是一个向量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其包含了<code>y^(1)</code>到<code>y^(m)</code>的值</li>
</ul>
<p><strong>正规方程求解的方法问题在于:</strong></p>
<p>正规方程需要计算矩阵<code>X^T · X</code>的逆<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它是一个<code>n * n</code>的矩阵<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><code>n</code>是特征的个数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这样一个矩阵求逆的运算复杂度大约在<code>O(n^2.4)</code>到<code>O(n^3)</code>之间<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>具体值取决于计算方式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>换句话说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果你将你的特征个数翻倍的话<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其计算时间大概会变为原来的 5.3<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><code>2^2.4</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>到 8<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><code>2^3</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>倍<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<blockquote>
<p>提示</p>
<p>当特征的个数较大的时候<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>特征数量为 100000<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>正规方程求解将会非常慢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</blockquote>
<p>有利的一面是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这个方程在训练集上对于每一个实例来说是线性的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其复杂度为<code>O(m)</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因此只要有能放得下它的内存空间<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它就可以对大规模数据进行训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>同时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一旦你得到了线性回归模型<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>通过解正规方程或者其他的算法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>进行预测是非常快的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>因为模型中计算复杂度对于要进行预测的实例数量和特征个数都是线性的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 换句话说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>当实例个数变为原来的两倍多的时候<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>或特征个数变为原来的两倍多<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>预测时间也仅仅是原来的两倍多<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h2 id="如何训练模型-梯度下降"><a href="#如何训练模型-梯度下降" class="headerlink" title="如何训练模型?梯度下降"></a>如何训练模型?梯度下降</h2><p>梯度下降是一种非常通用的优化算法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它能够很好地解决一系列问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>梯度下降的整体思路是通过的迭代来逐渐调整参数使得损失函数达到最小值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>假设浓雾下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你迷失在了大山中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你只能感受到自己脚下的坡度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>为了最快到达山底<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一个最好的方法就是沿着坡度最陡的地方下山<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这其实就是梯度下降所做的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>它计算误差函数关于参数向量<code>Θ</code>的局部梯度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>同时它沿着梯度下降的方向进行下一次迭代<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>当梯度值为零的时候<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>就达到了误差函数最小值 <span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>具体来说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>开始时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>需要选定一个随机的<code>Θ</code><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>这个值称为随机初始值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后逐渐去改进它<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>每一次变化一小步<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>每一步都试着降低损失函数<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>均方差损失函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>直到算法收敛到一个最小值<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>如图<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>4-3<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/%E5%9B%BE4-3.PNG"></p>
<p><strong>学习率learning rate</strong></p>
<p>在梯度下降中一个重要的参数是步长<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>超参数学习率的值决定了步长的大小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>如果学习率太小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>必须经过多次迭代<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>算法才能收敛<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这是非常耗时的<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>如图4-4<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br><img src="/img/loading.gif" data-original="/img/%E5%9B%BE4-4.PNG"></p>
<p>另一方面<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果学习率太大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你将跳过最低点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>到达山谷的另一面<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可能下一次的值比上一次还要大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这可能使的算法是发散的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>函数值变得越来越大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>永远不可能找到一个好的答案<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>如图4-5<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/%E5%9B%BE4-5.PNG"></p>
<p>最后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并不是所有的损失函数看起来都像一个规则的碗<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>它们可能是洞<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>山脊<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>高原和各种不规则的地形<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使它们收敛到最小值非常的困难<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 图4-6 显示了梯度下降的两个主要挑战<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>如果随机初始值选在了图像的左侧<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>则它将收敛到局部最小值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这个值要比全局最小值要大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 如果它从右侧开始<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>那么跨越高原将需要很长时间<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果你早早地结束训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你将永远到不了全局最小值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/%E5%9B%BE4-6.PNG"></p>
<p>事实上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>损失函数的图像呈现碗状<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是不同特征的取值范围相差较大的时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这个碗可能是细长的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>图4-7 展示了梯度下降在不同训练集上的表现<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>在左图中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>特征 1 和特征 2 有着相同的数值尺度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>在右图中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>特征 1 比特征 2 的取值要小的多<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>由于特征 1 较小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因此损失函数改变时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><code>Θ[1]</code>会有较大的变化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>于是这个图像会在<code>Θ[1]</code>轴方向变得细长<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/%E5%9B%BE4-7.PNG"></p>
<p>当我们使用梯度下降的时候<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>应该确保所有的特征有着相近的尺度范围<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>使用 Scikit Learn 的 <code>StandardScaler</code>类<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>否则它将需要很长的时间才能够收敛<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h3 id="批量梯度下降"><a href="#批量梯度下降" class="headerlink" title="批量梯度下降"></a>批量梯度下降</h3><p>使用梯度下降的过程中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你需要计算每一个<code>Θ[j]</code>下损失函数的梯度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>换句话说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你需要计算当<code>Θ[j]</code>变化一点点时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>损失函数改变了多少<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这称为偏导数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它就像当你面对东方的时候问<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>“我脚下的坡度是多少<span class="bd-box"><h-char class="bd bd-beg"><h-inner>？</h-inner></h-char></span>“<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>然后面向北方的时候问同样的问题<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>如果你能想象一个超过三维的宇宙<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以对所有的方向都这样做<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>公式 4-5 计算关于<code>Θ[j]</code>的损失函数的偏导数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>记为<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>∂MSE/∂θ[j]</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/tex-3a877201402d7cd2b9d3f5b726d22b24.gif"></p>
<p>为了避免单独计算每一个梯度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你也可以使用公式 4-6 来一起计算它们<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>梯度向量记为<code>ᐁ[θ]MSE(θ)</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其包含了损失函数所有的偏导数<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>每个模型参数只出现一次<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/tex-a007d1162d9c4957e8336b4b10d5fda3.gif"></p>
<h3 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h3><p>批量梯度下降的最要问题是计算每一步的梯度时都需要使用整个训练集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这导致在规模较大的数据集上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其会变得非常的慢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>与其完全相反的随机梯度下降<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在每一步的梯度计算上只随机选取训练集中的一个样本<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>很明显<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>由于每一次的操作都使用了非常少的数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这样使得算法变得非常快<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>由于每一次迭代<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>只需要在内存中有一个实例<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这使随机梯度算法可以在大规模训练集上使用<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>虽然随机性可以很好的跳过局部最优值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但同时它却不能达到最小值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>解决这个难题的一个办法是逐渐降低学习率<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 开始时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>走的每一步较大<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>这有助于快速前进同时跳过局部最小值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后变得越来越小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>从而使算法到达全局最小值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 这个过程被称为模拟退火<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因为它类似于熔融金属慢慢冷却的冶金学退火过程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 决定每次迭代的学习率的函数称为<code>learning schedule</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 如果学习速度降低得过快<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你可能会陷入局部最小值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>甚至在到达最小值的半路就停止了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 如果学习速度降低得太慢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你可能在最小值的附近长时间摆动<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>同时如果过早停止训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>最终只会出现次优解<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>由于每个实例的选择是随机的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>有的实例可能在每一代中都被选到<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这样其他的实例也可能一直不被选到<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>如果你想保证每一代迭代过程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>算法可以遍历所有实例<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一种方法是将训练集打乱重排<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后选择一个实例<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>之后再继续打乱重排<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>以此类推一直进行下去<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>但是这样收敛速度会非常的慢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h3 id="小批量梯度下降"><a href="#小批量梯度下降" class="headerlink" title="小批量梯度下降"></a>小批量梯度下降</h3><p>最后一个梯度下降算法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们将介绍小批量梯度下降算法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>一旦你理解了批量梯度下降和随机梯度下降<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>再去理解小批量梯度下降是非常简单的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>在迭代的每一步<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>批量梯度使用整个训练集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>随机梯度时候用仅仅一个实例<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在小批量梯度下降中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它则使用一个随机的小型实例集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>它比随机梯度的主要优点在于你可以通过矩阵运算的硬件优化得到一个较好的训练表现<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>尤其当你使用 GPU 进行运算的时候<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h2 id="学习曲线-x2F-过拟合-x2F-欠拟合"><a href="#学习曲线-x2F-过拟合-x2F-欠拟合" class="headerlink" title="学习曲线&#x2F;过拟合&#x2F;欠拟合"></a>学习曲线&#x2F;过拟合&#x2F;欠拟合</h2><p>如果你使用一个高阶的多项式回归<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你可能发现它的拟合程度要比普通的线性回归要好的多<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>图4-14 使用一个 300 阶的多项式模型去拟合之前的数据集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并同简单线性回归<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>2 阶的多项式回归进行比较<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>注意 300 阶的多项式模型如何摆动以尽可能接近训练实例<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/%E5%9B%BE4-14.PNG"></p>
<p>当然<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这种高阶多项式回归模型在这个训练集上严重过拟合了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>线性模型则欠拟合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>在这个训练集上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>二次模型有着较好的泛化能力<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>那是因为在生成数据时使用了二次模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是一般我们不知道这个数据生成函数是什么<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>那我们该如何决定我们模型的复杂度呢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>？</h-inner></h-char></span>你如何告诉我你的模型是过拟合还是欠拟合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>？</h-inner></h-char></span></p>
<ul>
<li><p>如果一个模型在训练集上表现良好<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>通过交叉验证指标却得出其泛化能力很差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>那么你的模型就是过拟合了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>如果在这两方面都表现不好<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>那么它就是欠拟合了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这种方法可以告诉我们<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你的模型是太复杂还是太简单了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</li>
<li><p>画出模型在训练集上的表现<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>同时画出以训练集规模为自变量的训练集函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>为了得到图像<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>需要在训练集的不同规模子集上进行多次训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>下面的代码定义了一个函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>用来画出给定训练集后的模型学习曲线</p>
</li>
</ul>
<p><img src="/img/loading.gif" data-original="/img/%E5%9B%BE4-15.PNG"></p>
<p><strong>上面的曲线表现了一个典型的欠拟合模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>两条曲线都到达高原地带并趋于稳定<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并且最后两条曲线非常接近<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>同时误差值非常大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></strong></p>
<p>这幅图值得我们深究<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>首先<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们观察训练集的表现<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>当训练集只有一两个样本的时候<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>模型能够非常好的拟合它们<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这也是为什么曲线是从零开始的原因<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>但是当加入了一些新的样本的时候<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>训练集上的拟合程度变得难以接受<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>出现这种情况有两个原因<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一是因为数据中含有噪声<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>另一个是数据根本不是线性的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>因此随着数据规模的增大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>误差也会一直增大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>直到达到高原地带并趋于稳定<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在之后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>继续加入新的样本<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>模型的平均误差不会变得更好或者更差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>我们继续来看模型在验证集上的表现<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>当以非常少的样本去训练时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>模型不能恰当的泛化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>也就是为什么验证误差一开始是非常大的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>当训练样本变多的到时候<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>模型学习的东西变多<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>验证误差开始缓慢的下降<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>但是一条直线不可能很好的拟合这些数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因此最后误差会到达在一个高原地带并趋于稳定<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>最后和训练集的曲线非常接近<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/%E5%9B%BE4-16.PNG"></p>
<p>这幅图像和之前的有一点点像<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是其有两个非常重要的不同点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<ul>
<li><p>在训练集上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>误差要比线性回归模型低的多<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</li>
<li><p>图中的两条曲线之间有间隔<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这意味模型在训练集上的表现要比验证集上好的多<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这也是模型过拟合的显著特点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>当然<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果你使用了更大的训练数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这两条曲线最后会非常的接近<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</li>
</ul>
<h3 id="误差来源分析"><a href="#误差来源分析" class="headerlink" title="误差来源分析"></a>误差来源分析</h3><blockquote>
<p>改善模型过拟合的一种方法是提供更多的训练数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>直到训练误差和验证误差相等<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br>在统计和机器学习领域有个重要的理论<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>一个模型的泛化误差由三个不同误差的和决定<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
</blockquote>
<ul>
<li><p>偏差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>泛化误差的这部分误差是由于错误的假设决定的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>例如实际是一个二次模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你却假设了一个线性模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>一个高偏差的模型最容易出现欠拟合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</li>
<li><p>方差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>这部分误差是由于模型对训练数据的微小变化较为敏感<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一个多自由度的模型更容易有高的方差<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>例如一个高阶多项式模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因此会导致模型过拟合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</li>
<li><p>不可约误差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>这部分误差是由于数据本身的噪声决定的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>降低这部分误差的唯一方法就是进行数据清洗<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>修复数据源<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>修复坏的传感器<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>识别和剔除异常值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</li>
</ul>
<p><img src="/img/loading.gif" data-original="/images/Pasted%20image%2020230209232627.png"><br><img src="/img/loading.gif" data-original="/images/Pasted%20image%2020230209232647.png"></p>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p><strong>降低模型的过拟合的好方法是正则化这个模型</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>即限制它<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>模型有越少的自由度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>就越难以拟合数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>正则化一个多项式模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一个简单的方法就是减少多项式的阶数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>对于一个线性模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>正则化的典型实现就是约束模型中参数的权重<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 接下来我们将介绍<strong>三种不同约束权重的方法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>Ridge 回归<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>Lasso 回归和 Elastic Net<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></strong></p>
<h3 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h3><p>岭回归<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>也称为 Tikhonov 正则化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>是线性回归的正则化版<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>在损失函数上直接加上一个正则项<code>α Σ θ[i]^2, i = 1 -&gt; n</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这使得学习算法不仅能够拟合数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而且能够使模型的参数权重尽量的小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>注意到这个正则项只有在训练过程中才会被加到损失函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>当得到完成训练的模型后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们应该使用没有正则化的测量方法去评价模型的表现<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<blockquote>
<p><em>训练过程使用的损失函数和测试过程使用的评价函数是不一样的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>除了正则化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>还有一个不同<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>训练时的损失函数应该在优化过程中易于求导</em></p>
</blockquote>
<p>岭回归损失函数:<br><img src="/img/loading.gif" data-original="/img/tex-de03ddd330336d12e33df21217bdab9d.gif"><br>超参数<code>α</code>决定了你想正则化这个模型的强度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>如果<code>α = 0</code>那此时的岭回归便变为了线性回归<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>如果<code>α</code>非常的大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所有的权重最后都接近于零<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>最后结果将是一条穿过数据平均值的水平直线<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>对于梯度下降来说仅仅在均方差梯度向量<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>公式 4-6<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>加上一项<code>αw</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/%E5%9B%BE4-17.PNG"><br>图4-17 展示了在相同线性数据上使用不同<code>α</code>值的岭回归模型最后的表现<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>左图中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使用简单的岭回归模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>最后得到了线性的预测<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>右图中的数据首先使用 10 阶的<code>PolynomialFearures</code>进行扩展<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后使用<code>StandardScaler</code>进行缩放<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>最后将岭模型应用在处理过后的特征上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这就是带有岭正则项的多项式回归<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>注意当<code>α</code>增大的时候<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>导致预测曲线变得扁平<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>即少了极端值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>多了一般值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这样减少了模型的方差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>却增加了模型的偏差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h3 id="Lasso回归"><a href="#Lasso回归" class="headerlink" title="Lasso回归"></a>Lasso回归</h3><p>Lasso 回归<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>也称 Least Absolute Shrinkage<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>或者 Selection Operator Regression<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>是另一种正则化版的线性回归<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>就像岭回归那样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它也在损失函数上添加了一个正则化项<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是它使用权重向量的<code>l1</code>范数而不是权重向量<code>l2</code>范数平方的一半<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>如公式 4-10<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p>
<p>Lasso 回归的损失函数:<br><img src="/img/loading.gif" data-original="/img/tex-a78e85b9c0eb6446f86c17d6d2190b74.gif"></p>
<p><img src="/img/loading.gif" data-original="/img/%E5%9B%BE4-18.PNG"><br>图4-18 展示了和图4-17 相同的事情<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在相同线性数据上使用不同<code>α</code>值的岭回归模型最后的表现<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>左图中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使用简单的岭回归模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>最后得到了线性的预测<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>右图中的数据首先使用 10 阶的<code>PolynomialFearures</code>进行扩展<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后使用<code>StandardScaler</code>进行缩放<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>最后用 Lasso 模型代替了 Ridge 模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>同时调小了<code>α</code>的值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><strong>Lasso回归的一个重要特征是它倾向于完全消除最不重要的特征的权重</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>即将它们设置为零<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>右图中的虚线所示<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><code>α = 10^(-7)</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>曲线看起来像一条二次曲线<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而且几乎是线性的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这是因为所有的高阶多项特征都被设置为零<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>换句话说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>Lasso 回归自动的进行特征选择同时输出一个稀疏模型</p>
<h3 id="弹性网络（ElasticNet）"><a href="#弹性网络（ElasticNet）" class="headerlink" title="弹性网络（ElasticNet）"></a>弹性网络<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>ElasticNet<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></h3><p>弹性网络介于 Ridge 回归和 Lasso 回归之间<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>它的正则项是 Ridge 回归和 Lasso 回归正则项的简单混合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>同时你可以控制它们的混合率<code>r</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>当<code>r = 0</code>时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>弹性网络就是 Ridge 回归<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>当<code>r = 1</code>时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其就是 Lasso 回归<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>具体表示如公式 4-12<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>弹性网络损失函数<br><img src="/img/loading.gif" data-original="/img/tex-e4da079f692fe35778bbdf1fdf120d99.gif"></p>
<blockquote>
<p>那么我们该如何选择线性回归<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>岭回归<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>Lasso 回归<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>弹性网络呢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>？</h-inner></h-char></span>一般来说有一点正则项的表现更好<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因此通常你应该避免使用简单的线性回归<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>岭回归是一个很好的首选项<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是如果你的特征仅有少数是真正有用的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你应该选择 Lasso 和弹性网络<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>就像我们讨论的那样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它两能够将无用特征的权重降为零<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>一般来说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>弹性网络的表现要比 Lasso 好<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因为当特征数量比样本的数量大的时候<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>或者特征之间有很强的相关性时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>Lasso 可能会表现的不规律<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</blockquote>
<h3 id="EarlyStopping"><a href="#EarlyStopping" class="headerlink" title="_EarlyStopping"></a>_EarlyStopping</h3><p>对于迭代学习算法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>有一种非常特殊的正则化方法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>就像梯度下降在验证错误达到最小值时立即停止训练那样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>我们称为早期停止法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>图4-20 表示使用批量梯度下降来训练一个非常复杂的模型<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>一个高阶多项式回归模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>随着训练的进行<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>算法一直学习<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它在训练集上的预测误差<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>RMSE<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>自然而然的下降<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>然而一段时间后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>验证误差停止下降<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并开始上升<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这意味着模型在训练集上开始出现过拟合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>一旦验证错误达到最小值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>便提早停止训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/%E5%9B%BE4-20.PNG"></p>
<h2 id="机器学习基础"><a href="#机器学习基础" class="headerlink" title="机器学习基础"></a>机器学习基础</h2><p><img src="/img/loading.gif" data-original="/images/Pasted%20image%2020230209233825.png"></p>
<p><img src="/img/loading.gif" data-original="/images/Pasted%20image%2020230209233842.png"></p>
<h2 id="支持向量机SVM"><a href="#支持向量机SVM" class="headerlink" title="支持向量机SVM"></a>支持向量机SVM</h2><h3 id="线性SVM分类"><a href="#线性SVM分类" class="headerlink" title="线性SVM分类"></a>线性SVM分类</h3><p>支持向量机<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>SVM<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>是个非常强大并且有多种功能的机器学习模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>能够做线性或者非线性的分类<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>回归<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>甚至异常值检测<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>机器学习领域中最为流行的模型之一<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>是任何学习机器学习的人必备的工具<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>SVM 特别适合应用于复杂但中小规模数据集的分类问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>SVM 的基本思想能够用一些图片来解释得很好<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><em>图 5-1 展示了我们在第 4 章结尾处介绍的鸢尾花数据集的一部分</em>这两个种类能够被非常清晰<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>非常容易的用一条直线分开<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>即线性可分的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>左边的图显示了三种可能的线性分类器的判定边界<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>其中用虚线表示的线性模型判定边界很差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>甚至不能正确地划分类别<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>另外两个线性模型在这个数据集表现的很好<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是它们的判定边界很靠近样本点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在新的数据上可能不会表现的很好<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>相比之下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>右边图中 SVM 分类器的判定边界实线<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不仅分开了两种类别<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而且还尽可能地远离了最靠近的训练数据点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><strong>你可以认为 SVM 分类器在两种类别之间保持了一条尽可能宽敞的街道<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>图中平行的虚线<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其被称为最大间隔分类<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>注意到添加更多的样本点在<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>街道<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char></span>外并不会影响到判定边界<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因为判定边界是由位于<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>街道<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char></span>边缘的样本点确定的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这些样本点被称为<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>支持向量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char></span></strong></p>
<p><img src="/img/loading.gif" data-original="/img/5-1.jpg"></p>
<p>SVM 对特征缩放比较敏感<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以看到图 5-2<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>左边的图中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>垂直的比例要更大于水平的比例<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以最宽的<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>街道<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char></span>接近水平<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>但对特征缩放后<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>例如使用 Scikit-Learn 的 StandardScaler<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>判定边界看起来要好得多<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如右图<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/5-2.jpg"></p>
<h4 id="软间隔分类"><a href="#软间隔分类" class="headerlink" title="软间隔分类"></a>软间隔分类</h4><p>如果我们严格地规定所有的数据都不在<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>街道<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char></span>上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>都在正确地两边<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>称为硬间隔分类.<br>硬间隔分类有两个问题</p>
<ol>
<li>只对线性可分的数据起作用<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span></li>
<li>对异常点敏感<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br>图 5-3 显示了只有一个异常点的鸢尾花数据集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>左边的图中很难找到硬间隔<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>右边的图中判定边界和我们之前在图 5-1 中没有异常点的判定边界非常不一样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它很难一般化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br><img src="/img/loading.gif" data-original="/img/5-3.jpg"></li>
</ol>
<p>为了避免上述的问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们更倾向于使用更加软性的模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>目的在保持<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>街道<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char></span>尽可能大和避免间隔违规<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>数据点出现在<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>街道<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char></span>中央或者甚至在错误的一边<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>之间找到一个良好的平衡<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这就是软间隔分类<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>在 Scikit-Learn 库的 SVM 类<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你可以用<code>C</code>超参数<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>惩罚系数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>来控制这种平衡<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>较小的<code>C</code>会导致更宽的<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>街道<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但更多的间隔违规<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>图 5-4 显示了在非线性可分隔的数据集上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>两个软间隔 SVM 分类器的判定边界<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>左边图中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使用了较大的<code>C</code>值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>导致更少的间隔违规<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是间隔较小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>右边的图<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使用了较小的<code>C</code>值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>间隔变大了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是许多数据点出现在了<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>街道<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char></span>上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>然而<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>第二个分类器似乎泛化地更好<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>事实上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在这个训练数据集上减少了预测错误<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因为实际上大部分的间隔违规点出现在了判定边界正确的一侧<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br><img src="/img/loading.gif" data-original="/img/5-4.jpg"></p>
<blockquote>
<p>如果你的 SVM 模型过拟合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你可以尝试通过减小超参数<code>C</code>去调整<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</blockquote>
<h3 id="非线性SVM分类"><a href="#非线性SVM分类" class="headerlink" title="非线性SVM分类"></a>非线性SVM分类</h3><p>尽管线性 SVM 分类器在许多案例上表现得出乎意料的好<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是很多数据集并不是线性可分的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>一种处理非线性数据集方法是增加更多的特征<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>例如多项式特征<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>正如你在第 4 章所做的那样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>在某些情况下可以变成线性可分的数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>在图 5-5 的左图中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它只有一个特征<code>x1</code>的简单的数据集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>正如你看到的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>该数据集不是线性可分的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>但是如果你增加了第二个特征 <code>x2=(x1)^2</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>产生的 2D 数据集就能很好的线性可分<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/5-5.jpg"></p>
<p>为了实施这个想法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>通过 Scikit-Learn<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你可以创建一个流水线<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>Pipeline<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>去包含多项式特征<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>PolynomialFeatures<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>变换<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>在 121 页的<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>Polynomial Regression<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char></span>中讨论<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后一个<code>StandardScaler</code>和<code>LinearSVC</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<pre><code class="python">from sklearn.datasets import make_moons
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures

polynomial_svm_clf = Pipeline((
        (&quot;poly_features&quot;, PolynomialFeatures(degree=3)),
        (&quot;scaler&quot;, StandardScaler()),
        (&quot;svm_clf&quot;, LinearSVC(C=10, loss=&quot;hinge&quot;))
    ))

polynomial_svm_clf.fit(X, y)
</code></pre>
<p><img src="/img/loading.gif" data-original="/img/5-6.jpg"></p>
<h4 id="多项式核"><a href="#多项式核" class="headerlink" title="多项式核"></a>多项式核</h4><p>添加多项式特征很容易实现<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不仅仅在 SVM<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在各种机器学习算法都有不错的表现<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是低次数的多项式不能处理非常复杂的数据集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而高次数的多项式却产生了大量的特征<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>会使模型变得慢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>幸运的是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>当你使用 SVM 时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你可以运用一个被称为<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>核技巧<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>kernel trick<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>的神奇数学技巧<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>它可以取得就像你添加了许多多项式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>甚至有高次数的多项式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一样好的结果<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>所以不会大量特征导致的组合爆炸<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因为你并没有增加任何特征<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<pre><code class="python">from sklearn.svm import SVC
poly_kernel_svm_clf = Pipeline((
        (&quot;scaler&quot;, StandardScaler()),
        (&quot;svm_clf&quot;, SVC(kernel=&quot;poly&quot;, degree=3, coef0=1, C=5))
    ))
poly_kernel_svm_clf.fit(X, y)
</code></pre>
<p>3 阶的多项式核训练了一个 SVM 分类器<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>即图 5-7 的左图<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>右图是使用了 10 阶的多项式核 SVM 分类器<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>很明显<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><strong>如果你的模型过拟合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你可以减小多项式核的阶数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>相反的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果是欠拟合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你可以尝试增大它<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>超参数<code>coef0</code>控制了高阶多项式与低阶多项式对模型的影响<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></strong></p>
<blockquote>
<p>通用的方法是用网格搜索<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>grid search 见第 2 章<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>去找到最优超参数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>首先进行非常粗略的网格搜索一般会很快<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后在找到的最佳值进行更细的网格搜索<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</blockquote>
<p><img src="/img/loading.gif" data-original="/img/5-7.jpg"></p>
<h4 id="增加相似特征"><a href="#增加相似特征" class="headerlink" title="增加相似特征"></a>增加相似特征</h4><p>另一种解决非线性问题的方法是使用相似函数<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>similarity funtion<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>计算每个样本与特定地标<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>landmark<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>的相似度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>让我们来看看前面讨论过的一维数据集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并在<code>x1=-2</code>和<code>x1=1</code>之间增加两个地标<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>图 5-8 左图<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>接下来<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们定义一个相似函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>即高斯径向基函数<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>Gaussian Radial Basis Function<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>RBF<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>设置<code>γ = 0.3</code><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>见公式 5-1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/tex-8b908429a5b5ee2e519f8caa16f82ee1.gif" alt="RBF高斯径向基函数"></p>
<p>你可能想知道如何选择地标<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>最简单的方法是在数据集中的每一个样本的位置创建地标<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这将产生更多的维度从而增加了转换后数据集是线性可分的可能性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>但缺点是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><code>m</code>个样本<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><code>n</code>个特征的训练集被转换成了<code>m</code>个实例<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><code>m</code>个特征的训练集<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>假设你删除了原始特征<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这样一来<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果你的训练集非常大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你最终会得到同样大的特征<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h4 id="高斯-RBF-核"><a href="#高斯-RBF-核" class="headerlink" title="高斯 RBF 核"></a>高斯 RBF 核</h4><p>就像多项式特征法一样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>相似特征法对各种机器学习算法同样也有不错的表现<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>但是在所有额外特征上的计算成本可能很高<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>特别是在大规模的训练集上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>然而<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>核<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char></span> 技巧再一次显现了它在 SVM 上的神奇之处<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>高斯核让你可以获得同样好的结果成为可能<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>就像你在相似特征法添加了许多相似特征一样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但事实上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你并不需要在 RBF 添加它们<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>我们使用 SVC 类的高斯 RBF 核来检验一下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<pre><code class="python">rbf_kernel_svm_clf = Pipeline((
        (&quot;scaler&quot;, StandardScaler()),
        (&quot;svm_clf&quot;, SVC(kernel=&quot;rbf&quot;, gamma=5, C=0.001))
    ))
rbf_kernel_svm_clf.fit(X, y)
</code></pre>
<p>这个模型在图 5-9 的左下角表示<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>其他的图显示了用不同的超参数<code>gamma (γ)</code>和<code>C</code>训练的模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>增大<code>γ</code>使钟型曲线更窄<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>图 5-8 左图<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>导致每个样本的影响范围变得更小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>即判定边界最终变得更不规则<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在单个样本周围环绕<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>相反的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>较小的<code>γ</code>值使钟型曲线更宽<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>样本有更大的影响范围<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>判定边界最终则更加平滑<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><strong>所以γ是可调整的超参数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>如果你的模型过拟合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你应该减小<code>γ</code>值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>若欠拟合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>则增大<code>γ</code><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>与超参数<code>C</code>相似<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></strong></p>
<p><img src="/img/loading.gif" data-original="/img/5-9.jpg"></p>
<h4 id="计算复杂度"><a href="#计算复杂度" class="headerlink" title="计算复杂度"></a>计算复杂度</h4><p><code>LinearSVC</code>类基于<code>liblinear</code>库<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它实现了线性 SVM 的优化算法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><strong>它并不支持核技巧<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是它样本和特征的数量几乎是线性的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>训练时间复杂度大约为<code>O(m × n)</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></strong></p>
<p>如果你要非常高的精度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这个算法需要花费更多时间<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这是由容差值超参数<code>ϵ</code><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>在 Scikit-learn 称为<code>tol</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>控制的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>大多数分类任务中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使用默认容差值的效果是已经可以满足一般要求<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><strong>SVC 类基于<code>libsvm</code>库<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它实现了支持核技巧的算法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>训练时间复杂度通常介<code>于 O(m^2 × n)</code>和<code>O(m^3 × n)</code>之间.</strong>  不幸的是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这意味着当训练样本变大时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它将变得极其慢<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>成千上万个样本<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这个算法对于复杂但小型或中等数量的数据集表现是完美的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>然而<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它能对特征数量很好的缩放<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>尤其对稀疏特征来说<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>sparse features<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>即每个样本都有一些非零特征<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>在这个情况下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>算法对每个样本的非零特征的平均数量进行大概的缩放<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>表 5-1 对 Scikit-learn 的 SVM 分类模型进行比较<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br><img src="/img/loading.gif" data-original="/img/tb-5-1.jpg"></p>
<h3 id="SVM回归"><a href="#SVM回归" class="headerlink" title="SVM回归"></a>SVM回归</h3><p>SVM 算法应用广泛<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>不仅仅支持线性和非线性的分类任务<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>还支持线性和非线性的回归任务<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>SVM回归核心在于逆转我们的目标<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><strong>限制间隔违规的情况下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不是试图在两个类别之间找到尽可能大的<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>街道<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>即间隔<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>SVM 回归任务是限制间隔违规情况下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>尽量放置更多的样本在<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>街道<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char></span>上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>街道<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char></span>的宽度由超参数<code>ϵ</code>控制<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>图 5-10 显示了在一些随机生成的线性数据上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>两个线性 SVM 回归模型的训练情况<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>一个有较大的间隔<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><code>ϵ=1.5</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>另一个间隔较小<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><code>ϵ=0.5</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span><br><img src="/img/loading.gif" data-original="/img/5-10.jpg"></p>
<p>你可以使用 Scikit-Learn 的<code>LinearSVR</code>类去实现线性 SVM 回归<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>下面的代码产生的模型在图 5-10 左图<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>训练数据需要被中心化和标准化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p>
<pre><code class="python">from sklearn.svm import LinearSVR
svm_reg = LinearSVR(epsilon=1.5)
svm_reg.fit(X, y)
</code></pre>
<p>处理非线性回归任务<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你可以使用核化的 SVM 模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>比如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>图 5-11 显示了在随机二次方的训练集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使用二次方多项式核函数的 SVM 回归<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>左图是较小的正则化<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>即更大的<code>C</code>值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>右图则是更大的正则化<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>即小的<code>C</code>值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span><br><img src="/img/loading.gif" data-original="/img/5-11.jpg"></p>
<pre><code class="python">from sklearn.svm import SVR

svm_poly_reg = SVR(kernel=&quot;poly&quot;, degree=2, C=100, epsilon=0.1)
svm_poly_reg.fit(X, y)
</code></pre>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>和支持向量机一样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 决策树是一种多功能机器学习算法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 即可以执行分类任务也可以执行回归任务<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 甚至包括多输出<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>multioutput<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>任务.它是一种功能很强大的算法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以对很复杂的数据集进行拟合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在第二章中我们对加利福尼亚住房数据集使用决策树回归模型进行训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>就很好的拟合了数据集<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>实际上是过拟合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>决策树也是随机森林的基本组成部分<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>见第 7 章<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而随机森林是当今最强大的机器学习算法之一<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><p>理解决策树<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们需要先构建一个决策树并亲身体验它到底如何进行预测<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 接下来的代码就是在我们熟知的鸢尾花数据集上进行一个决策树分类器的训练</p>
<pre><code class="python">from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
iris = load_iris()
X = iris.data[:, 2:] # petal length and width 
y = iris.target
tree_clf = DecisionTreeClassifier(max_depth=2)
tree_clf.fit(X, y)
</code></pre>
<p>我们的第一个决策树如图 6-1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/102.png"><br><strong>决策树的众多特性之一就是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 它不需要太多的数据预处理<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 尤其是不需要进行特征的缩放或者归一化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></strong></p>
<p>现在让我们来看看在图 6-1 中的树是如何进行预测的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>假设你找到了一朵鸢尾花并且想对它进行分类<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你从根节点开始<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>深度为 0<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>顶部<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>该节点询问花朵的花瓣长度是否小于 2.45 厘米<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>如果是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>您将向下移动到根的左侧子节点<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>深度为 1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>左侧<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 在这种情况下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它是一片叶子节点<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>即它没有任何子节点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以它不会问任何问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>你可以方便地查看该节点的预测类别<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>决策树预测你的花是 Iris-Setosa<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><code>class = setosa</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>现在假设你找到了另一朵花<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但这次的花瓣长度是大于 2.45 厘米的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>你必须向下移动到根的右侧子节点<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>深度为 1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>右侧<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而这个节点不是叶节点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以它会问另一个问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>花瓣宽度是否小于 1.75 厘米<span class="bd-box"><h-char class="bd bd-beg"><h-inner>？</h-inner></h-char></span> 如果是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>那么你的花很可能是一个 Iris-Versicolor<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>深度为 2<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>左<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 如果不是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>那很可能一个 Iris-Virginica<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>深度为 2<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>右<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/103.png"><br>图 6-2 显示了决策树的决策边界<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>粗的垂直线代表根节点<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>深度为 0<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>的决定边界<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>花瓣长度为 2.45 厘米<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>由于左侧区域是纯的<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>只有 Iris-Setosa<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以不能再进一步分裂<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>然而<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>右边的区域是不纯的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以深度为 1 的右边节点在花瓣宽度为 1.75 厘米处分裂<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>用虚线表示<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>又由于<code>max_depth</code>设置为 2<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>决策树在那里停了下来<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>但是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果将<code>max_depth</code>设置为 3<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>两个深度为 2 的节点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>每个都将会添加另一个决策边界<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>用虚线表示<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><em>决策树非常直观<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>他们的决定很容易被解释<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这种模型通常被称为白盒模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>相反<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>随机森林或神经网络通常被认为是黑盒模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>他们能做出很好的预测<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并且您可以轻松检查它们做出这些预测过程中计算的执行过程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>然而<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>人们通常很难用简单的术语来解释为什么模型会做出这样的预测<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></em></p>
<h4 id="CART训练算法"><a href="#CART训练算法" class="headerlink" title="_CART训练算法"></a>_CART训练算法</h4><p>Scikit-Learn 用分裂回归树<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>Classification And Regression Tree<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>简称 CART<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>算法训练决策树<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>也叫<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>增长树<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这种算法思想真的非常简单<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>首先使用单个特征<code>k</code>和阈值<code>t[k]</code><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>花瓣长度<code>≤2.45cm</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>将训练集分成两个子集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>它如何选择<code>k</code>和<code>t[k]</code>呢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>？</h-inner></h-char></span>它寻找到能够产生最纯粹的子集一对<code>(k, t[k])</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后通过子集大小加权计算<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>算法会尝试最小化成本函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/104.png"></p>
<p>当它成功的将训练集分成两部分之后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 它将会继续使用相同的递归式逻辑继续的分割子集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后是子集的子集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>当达到预定的最大深度之后将会停止分裂<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>由<code>max_depth</code>超参数决定<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>或者是它找不到可以继续降低不纯度的分裂方法的时候<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>几个其他超参数<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><code>min_samples_split</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><code>min_samples_leaf</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><code>min_weight_fraction_leaf</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><code>max_leaf_nodes</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>控制了其他的停止生长条件</p>
<p>找到最优树是一个 NP 完全问题<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>自行百度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>它需要<code>O(exp^m)</code>时间<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>即使对于相当小的训练集也会使问题变得棘手<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 这就是为什么我们必须设置一个<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>合理的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>而不是最佳的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>解决方案<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>在建立好决策树模型后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 做出预测需要遍历决策树<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 从根节点一直到叶节点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>决策树通常近似左右平衡<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因此遍历决策树需要经历大致<code>O(log2(m))</code> 个节点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>由于每个节点只需要检查一个特征的值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因此总体预测复杂度仅为<code>O(log2(m))</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>与特征的数量无关<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 所以即使在处理大型训练集时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>预测速度也非常快<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>然而<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>训练算法的时候<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>训练和预测不同<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>需要比较所有特征<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>如果设置了<code>max_features</code>会更少一些<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p>
<p>在每个节点的所有样本上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>就有了<code>O(n×m log(m))</code>的训练复杂度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>对于小型训练集<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>少于几千例<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>Scikit-Learn 可以通过预先设置数据<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><code>presort = True</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>来加速训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是这对于较大训练集来说会显着减慢训练速度</p>
<h4 id="正则化-1"><a href="#正则化-1" class="headerlink" title="正则化"></a>正则化</h4><p>如果不添加约束<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>树结构模型通常将根据训练数据调整自己<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使自身能够很好的拟合数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而这种情况下大多数会导致模型过拟合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><code>DecisionTreeClassifier</code>类还有一些其他的参数用于限制树模型的形状:</p>
<blockquote>
<p><code>min_samples_split</code><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>节点在被分裂之前必须具有的最小样本数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><code>min_samples_leaf</code><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>叶节点必须具有的最小样本数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><code>min_weight_fraction_leaf</code><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>和<code>min_samples_leaf</code>相同<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但表示为加权总数的一小部分实例<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><code>max_leaf_nodes</code><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>叶节点的最大数量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span><code>和 max_features</code><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>在每个节点被评估是否分裂的时候<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>具有的最大特征数量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>增加<code>min_* hyperparameters</code>或者减少<code>max_* hyperparameters</code>会使模型正则化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>一些其他算法的工作原理是在没有任何约束条件下训练决策树模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>让模型自由生长<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后再对不需要的节点进行剪枝<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</blockquote>
<p>图 6-3 显示了对<code>moons</code>数据集<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>在第 5 章介绍过<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>进行训练生成的两个决策树模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>左侧的图形对应的决策树使用默认超参数生成<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>没有限制生长条件<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>右边的决策树模型设置为<code>min_samples_leaf=4</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>很明显<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>左边的模型过拟合了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而右边的模型泛用性更好<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/105.png"></p>
<h3 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h3><p>决策树也能够执行回归任务<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>让我们使用 Scikit-Learn 的<code>DecisionTreeRegressor</code>类构建一个回归树<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>让我们用<code>max_depth = 2</code>在具有噪声的二次项数据集上进行训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<pre><code class="python">from sklearn.tree import DecisionTreeRegressor
tree_reg = DecisionTreeRegressor(max_depth=2)
tree_reg.fit(X, y)
</code></pre>
<p><img src="/img/loading.gif" data-original="/img/106.png"></p>
<p>这棵树看起来非常类似于你之前建立的分类树<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它的主要区别在于<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它不是预测每个节点中的样本所属的分类<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而是预测一个具体的数值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>假设您想对<code>x[1] = 0.6</code>的新实例进行预测<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>从根开始遍历树<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>最终到达预测值等于 0.1106 的叶节点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>该预测仅仅是与该叶节点相关的 110 个训练实例的平均目标值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>而这个预测结果在对应的 110 个实例上的均方误差<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>MSE<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>等于 0.0151<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/108.png"><br>CART 算法的工作方式与之前处理分类模型基本一样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不同之处在于<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>现在不再以最小化不纯度的方式分割训练集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而是试图以最小化 MSE 的方式分割训练集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>公式 6-4 显示了成本函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>该算法试图最小化这个成本函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/109.png"><br>和处理分类任务时一样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>决策树在处理回归问题的时候也容易过拟合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>如果不添加任何正则化<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>默认的超参数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你就会得到图 6-6 左侧的预测结果<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>显然<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>过度拟合的程度非常严重<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>而当我们设置了<code>min_samples_leaf = 10</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>相对就会产生一个更加合适的模型了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>就如图 6-6 所示的那样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h3 id="不稳定性"><a href="#不稳定性" class="headerlink" title="不稳定性"></a>不稳定性</h3><p>它很容易理解和解释<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>易于使用且功能丰富而强大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>然而<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它也有一些限制<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>首先<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你可能已经注意到了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>&#x3D;&#x3D;决策树很喜欢设定正交化的决策边界<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>&#x3D;&#x3D;<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>所有边界都是和某一个轴相垂直的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这使得它对训练数据集的旋转很敏感<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>例如图 6-7 显示了一个简单的线性可分数据集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>在左图中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>决策树可以轻易的将数据分隔开<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是在右图中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>当我们把数据旋转了 45° 之后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>决策树的边界看起来变的格外复杂<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>尽管两个决策树都完美的拟合了训练数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>右边模型的泛化能力很可能非常差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br><img src="/img/loading.gif" data-original="file://Users/kpy/Documents/hands-on-ml-2e-zh/docs/img/110.png?lastModify=1676195839"><br>解决这个难题的一种方式是使用 PCA 主成分分析<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>第八章<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这样通常能使训练结果变得更好一些<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/103.png"><br><img src="/img/loading.gif" data-original="/img/111.png"><br>更加通俗的讲<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>&#x3D;&#x3D;决策时的主要问题是它对训练数据的微小变化非常敏感<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>&#x3D;&#x3D;举例来说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们仅仅从鸢尾花训练数据中将最宽的 Iris-Versicolor 拿掉<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>花瓣长 4.8 厘米<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>宽 1.8 厘米<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后重新训练决策树模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你可能就会得到图 6-8 中的模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>正如我们看到的那样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>决策树有了非常大的变化<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>原来的如图 6-2<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>事实上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>由于 Scikit-Learn 的训练算法是非常随机的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>即使是相同的训练数据你也可能得到差别很大的模型<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>除非你设置了随机数种子<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h2 id="集成学习-x2F-随机森林"><a href="#集成学习-x2F-随机森林" class="headerlink" title="集成学习&#x2F;随机森林"></a>集成学习&#x2F;随机森林</h2><p>假设你去随机问很多人一个很复杂的问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后把它们的答案合并起来<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>通常情况下你会发现这个合并的答案比一个专家的答案要好<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这就叫做_群体智慧_<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>同样的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果你合并了一组分类器的预测<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>像分类或者回归<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你也会得到一个比单一分类器更好的预测结果<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这一组分类器就叫做集成<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>因此<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这个技术就叫做集成学习<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一个集成学习算法就叫做集成方法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>你可以训练一组决策树分类器<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>每一个都在一个随机的训练集上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>为了去做预测<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你必须得到所有单一树的预测值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后通过投票<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>例如第六章的练习<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>来预测类别<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>例如一种决策树的集成就叫做随机森林<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它除了简单之外也是现今存在的最强大的机器学习算法之一<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h3 id="投票-分类"><a href="#投票-分类" class="headerlink" title="投票(分类)"></a>投票(分类)</h3><p>假设你已经训练了一些分类器<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>每一个都有 80% 的准确率<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>你可能有了一个逻辑斯蒂回归<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>或一个 SVM<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>或一个随机森林<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>或者一个 KNN<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>或许还有更多<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>详见图 7-1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>一个非常简单去创建一个更好的分类器的方法就是去整合每一个分类器的预测然后经过投票去预测分类<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这种分类器就叫做硬投票分类器<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>详见图 7-2<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br>令人惊奇的是这种投票分类器得出的结果经常会比集成中最好的一个分类器结果更好<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>事实上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>即使每一个分类器都是一个弱学习器<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>意味着它们也就比瞎猜好点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>集成后仍然是一个强学习器<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>高准确率<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>只要有足够数量的弱学习者<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>他们就足够多样化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/7-1.png"><br><img src="/img/loading.gif" data-original="/img/7-2.png"></p>
<pre><code class="python">&gt;&gt;&gt; from sklearn.ensemble import RandomForestClassifier 
&gt;&gt;&gt; from sklearn.ensemble import VotingClassifier 
&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression 
&gt;&gt;&gt; from sklearn.svm import SVC
&gt;&gt;&gt; log_clf = LogisticRegression() 
&gt;&gt;&gt; rnd_clf = RandomForestClassifier() 
&gt;&gt;&gt; svm_clf = SVC()
&gt;&gt;&gt; voting_clf = VotingClassifier(estimators=[(&#39;lr&#39;, log_clf), (&#39;rf&#39;, rnd_clf), 
&gt;&gt;&gt;   (&#39;svc&#39;, svm_clf)],voting=&#39;hard&#39;) 
&gt;&gt;&gt; voting_clf.fit(X_train, y_train)
</code></pre>
<pre><code class="python">&gt;&gt;&gt; from sklearn.metrics import accuracy_score 
&gt;&gt;&gt; for clf in (log_clf, rnd_clf, svm_clf, voting_clf): 
&gt;&gt;&gt;     clf.fit(X_train, y_train) 
&gt;&gt;&gt;     y_pred = clf.predict(X_test) 
&gt;&gt;&gt;     print(clf.__class__.__name__, accuracy_score(y_test, y_pred)) 
LogisticRegression 0.864 
RandomForestClassifier 0.872 
SVC 0.888 
VotingClassifier 0.896 
</code></pre>
<p>如果所有的分类器都能够预测类别的概率<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>例如他们有一个<code>predict_proba()</code>方法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>那么你就可以让 sklearn 以最高的类概率来预测这个类<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>平均在所有的分类器上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这种方式叫做软投票<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>他经常比硬投票表现的更好<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因为它给予高自信的投票更大的权重<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>你可以通过把<code>voting=&quot;hard&quot;</code>设置为<code>voting=&quot;soft&quot;</code>来保证分类器可以预测类别概率<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h3 id="Bagging-amp-Pasting"><a href="#Bagging-amp-Pasting" class="headerlink" title="Bagging &amp; Pasting"></a>Bagging &amp; Pasting</h3><p>可以通过使用不同的训练算法去得到一些不同的分类器<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><strong>另一种方法就是对每一个分类器都使用相同的训练算法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是在不同的训练集上去训练它们<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>有放回采样被称为装袋<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><em>Bagging</em><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>是 <em>bootstrap aggregating</em> 的缩写<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>无放回采样称为粘贴<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><em>pasting</em><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></strong> Bagging 和 Pasting 都允许在多个分类器上对训练集进行多次采样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但只有 Bagging 允许对同一种分类器上对训练集进行进行多次采样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>采样和训练过程如图 7-4 所示<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/7-4.png"></p>
<p>当所有的分类器被训练后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>集成可以通过对所有分类器结果的简单聚合来对新的实例进行预测<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>聚合函数通常对分类是_统计模式_<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>例如硬投票分类器<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>或者对回归是平均<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>每一个单独的分类器在如果在原始训练集上都是高偏差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是聚合降低了偏差和方差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>通常情况下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>集成的结果是有一个相似的偏差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是对比与在原始训练集上的单一分类器来讲有更小的方差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<blockquote>
<p> 分类器可以通过不同的 CPU 核或其他的服务器一起被训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>相似的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>分类器也可以一起被制作<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这就是为什么 Bagging 和 Pasting 是如此流行的原因之一<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>它们的可扩展性很好<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</blockquote>
<p>sklearn 为 Bagging 和 Pasting 提供了一个简单的 API<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>BaggingClassifier</code>类<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>或者对于回归可以是<code>BaggingRegressor</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>接下来的代码训练了一个 500 个决策树分类器的集成<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>每一个都是在数据集上有放回采样 100 个训练实例下进行训练<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>这是 Bagging 的例子<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果你想尝试 Pasting<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>就设置<code>bootstrap=False</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><code>n_jobs</code>参数告诉 sklearn 用于训练和预测所需要 CPU 核的数量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>-1 代表着 sklearn 会使用所有空闲核<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<pre><code class="python">&gt;&gt;&gt;from sklearn.ensemble import BaggingClassifier 
&gt;&gt;&gt;from sklearn.tree import DecisionTreeClassifier
&gt;&gt;&gt;bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,        
&gt;&gt;&gt;  max_samples=100, bootstrap=True, n_jobs=-1) 
&gt;&gt;&gt;bag_clf.fit(X_train, y_train) 
&gt;&gt;&gt;y_pred = bag_clf.predict(X_test)
</code></pre>
<p>Bootstrap 在每个预测器被训练的子集中引入了更多的分集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以 Bagging 结束时的偏差比 Pasting 更高<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但这也意味着预测因子最终变得不相关<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>从而减少了集合的方差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>总体而言<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>Bagging 通常会导致更好的模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这就解释了为什么它通常是首选的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br><img src="/img/loading.gif" data-original="/img/7-5.png"></p>
<p>对于 Bagging 来说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一些实例可能被一些分类器重复采样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但其他的有可能不会被采样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><code>BaggingClassifier</code>默认采样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><code>BaggingClassifier</code>默认是有放回的采样<code>m</code>个实例 <span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><code>bootstrap=True</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其中<code>m</code>是训练集的大小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这意味着平均下来只有 63% 的训练实例被每个分类器采样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>剩下的 37% 个&#x3D;&#x3D;没有被采样的训练实例就叫做 <em>Out-of-Bag</em> 实例<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>&#x3D;&#x3D;</p>
<p>在 sklearn 中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你可以在训练后需要创建一个<code>BaggingClassifier</code>来自动评估时设置<code>oob_score=True</code>来自动评估<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>接下来的代码展示了这个操作<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>评估结果通过变量<code>oob_score_</code>来显示<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<pre><code class="python">&gt;&gt;&gt; bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,bootstrap=True, n_jobs=-1, oob_score=True)
&gt;&gt;&gt; bag_clf.fit(X_train, y_train) 
&gt;&gt;&gt; bag_clf.oob_score_ 
0.93066666666666664 
</code></pre>
<h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><p>正如我们所讨论的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><strong>随机森林是决策树的一种集成</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>通常是通过 bagging 方法<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>有时是 pasting 方法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>进行训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>通常用<code>max_samples</code>设置为训练集的大小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>与建立一个<code>BaggingClassifier</code>然后把它放入<code>DecisionTreeClassifier</code>相反, 你可以使用更方便的也是对决策树优化够的<code>RandomForestClassifier</code><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>对于回归是<code>RandomForestRegressor</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>接下来的代码训练了带有 500 个树<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>每个被限制为 16 叶子结点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>的决策森林</p>
<pre><code class="python">&gt;&gt;&gt;from sklearn.ensemble import RandomForestClassifier
&gt;&gt;&gt;rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1) 
&gt;&gt;&gt;rnd_clf.fit(X_train, y_train)
&gt;&gt;&gt;y_pred_rf = rnd_clf.predict(X_test)
</code></pre>
<pre><code class="python">&gt;&gt;&gt;from sklearn.ensemble import RandomForestClassifier
&gt;&gt;&gt;rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1) 
&gt;&gt;&gt;rnd_clf.fit(X_train, y_train)
&gt;&gt;&gt;y_pred_rf = rnd_clf.predict(X_test)
</code></pre>
<p>除了一些例外<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><code>RandomForestClassifier</code>使用<code>DecisionTreeClassifier</code>的所有超参数<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>决定数怎么生长<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>把<code>BaggingClassifier</code>的超参数加起来来控制集成本身<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br>随机森林算法在树生长时引入了额外的随机<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>与在节点分裂时需要找到最好分裂特征相反<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>详见第六章<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它在一个随机的特征集中找最好的特征<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>它导致了树的差异性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并且再一次用高偏差换低方差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>总的来说是一个更好的模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<ul>
<li>极随机树<br>当你在随机森林上生长树时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在每个结点分裂时只考虑随机特征集上的特征<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>正如之前讨论过的一样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>相比于找到更好的特征我们可以通过使用对特征使用随机阈值使树更加随机<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>像规则决策树一样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br>这种极随机的树被简称为 <em>Extremely Randomized Trees</em><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>极随机树<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>或者更简单的称为 <em>Extra-Tree</em><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>再一次用高偏差换低方差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>它还使得 <em>Extra-Tree</em> 比规则的随机森林更快地训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因为在每个节点上找到每个特征的最佳阈值是生长树最耗时的任务之一<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></li>
</ul>
<p>最后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果你观察一个单一决策树<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>重要的特征会出现在更靠近根部的位置<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而不重要的特征会经常出现在靠近叶子的位置<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>因此我们可以通过计算一个特征在森林的全部树中出现的平均深度来预测特征的重要性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>sklearn 在训练后会自动计算每个特征的重要度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>你可以通过<code>feature_importances_</code>变量来查看结果<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<pre><code class="python">&gt;&gt;&gt; from sklearn.datasets import load_iris 
&gt;&gt;&gt; iris = load_iris() 
&gt;&gt;&gt; rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1) 
&gt;&gt;&gt; rnd_clf.fit(iris[&quot;data&quot;], iris[&quot;target&quot;]) 
&gt;&gt;&gt; for name, score in zip(iris[&quot;feature_names&quot;], rnd_clf.feature_importances_): 
&gt;&gt;&gt;     print(name, score) 
sepal length (cm) 0.112492250999
sepal width (cm) 0.0231192882825 
petal length (cm) 0.441030464364 
petal width (cm) 0.423357996355 
</code></pre>
<p>相似的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果你在 MNIST 数据及上训练随机森林分类器<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>在第三章上介绍<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后画出每个像素的重要性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你可以得到图 7-6 的图片<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br><img src="/img/loading.gif" data-original="/img/7-6.png"></p>
<h3 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h3><p><strong>提升<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>Boosting<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>最初称为 <em>假设增强</em> <span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>指的是可以将几个弱学习者组合成强学习者的集成方法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>对于大多数的提升方法的思想就是按顺序去训练分类器<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>每一个都要尝试修正前面的分类<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></strong> 现如今已经有很多的提升方法了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但最著名的就是 <em>Adaboost</em><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>适应性提升<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>是 <em>Adaptive Boosting</em> 的简称<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span> 和 <em>Gradient Boosting</em><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>梯度提升<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>让我们先从 <em>Adaboost</em> 说起<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h4 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h4><p>使一个新的分类器去修正之前分类结果的方法就是对之前分类结果不对的训练实例多加关注<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这导致新的预测因子越来越多地聚焦于这种情况<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这是 <em>Adaboost</em> 使用的技术<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>举个例子<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>去构建一个 Adaboost 分类器<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>第一个基分类器<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>例如一个决策树<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>被训练然后在训练集上做预测<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在误分类训练实例上的权重就增加了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>第二个分类机使用更新过的权重然后再一次训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>权重更新<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>以此类推<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>详见图 7-7<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span><br><img src="/img/loading.gif" data-original="/img/7-7.png"><br><img src="/img/loading.gif" data-original="/img/7-8.png"></p>
<blockquote>
<p> 一旦所有的分类器都被训练后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>除了分类器根据整个训练集上的准确率被赋予的权重外<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>集成预测就非常像 Bagging 和 Pasting 了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>序列学习技术的一个重要的缺点就是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>它不能被并行化<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>只能按步骤<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因为每个分类器只能在之前的分类器已经被训练和评价后再进行训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>因此<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它不像 Bagging 和 Pasting 一样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</blockquote>
<p>让我们详细看一下 Adaboost 算法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>每一个实例的权重<code>wi</code>初始都被设为<code>1/m</code>第一个分类器被训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后他的权重误差率<code>r1</code>在训练集上算出<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>详见公式 7-1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>其中<code>y_tilde[j]^(i)</code>是第<code>j</code>个分类器对于第<code>i</code>实例的预测<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<ul>
<li>实例的权重<br><img src="/img/loading.gif" data-original="/img/E7-1.png" alt="实例权重"></li>
</ul>
<p>分类器的权重<code>α[j]</code>随后用公式 7-2 计算出来<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>其中<code>η</code>是超参数学习率<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>默认为 1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>分类器准确率越高<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它的权重就越高<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>如果它只是瞎猜<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>那么它的权重会趋近于 0<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>然而<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果它总是出错<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>比瞎猜的几率都低<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它的权重会使负数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<ul>
<li>分类器的权重<br><img src="/img/loading.gif" data-original="/img/E7-2.png"></li>
</ul>
<p>接下来实例的权重会按照公式 7-3 更新<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>误分类的实例权重会被提升<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<ul>
<li>权重更新规则<br><img src="/img/loading.gif" data-original="/img/E7-3.png"></li>
</ul>
<p>随后所有实例的权重都被归一化<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>例如被<code>Σ w[i], i = 1 -&gt; m</code>整除<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span><br>最后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一个新的分类器通过更新过的权重训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>整个过程被重复<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>新的分类器权重被计算<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>实例的权重被更新<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>随后另一个分类器被训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>以此类推<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>当规定的分类器数量达到或者最好的分类器被找到后算法就会停止<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br>为了进行预测<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>Adaboost 通过分类器权重<code>α[j]</code>简单的计算了所有的分类器和权重<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>预测类别会是权重投票中主要的类别<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>详见公式 7-4<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>其中<code>N</code>是分类器的数量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<ul>
<li>AdaBoost 分类器<br><img src="/img/loading.gif" data-original="/img/E7-4.png"><br>下来的代码训练了使用 sklearn 的<code>AdaBoostClassifier</code>基于 200 个决策树桩 Adaboost 分类器<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>正如你说期待的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>对于回归也有<code>AdaBoostRegressor</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>一个决策树桩是<code>max_depth=1</code>的决策树  是一个单一的决策节点加上两个叶子结点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这就是<code>AdaBoostClassifier</code>的默认基分类器<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></li>
</ul>
<pre><code class="python">&gt;&gt;&gt;from sklearn.ensemble import AdaBoostClassifier
&gt;&gt;&gt;ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200,algorithm=&quot;SAMME.R&quot;, learning_rate=0.5) 
&gt;&gt;&gt;ada_clf.fit(X_train, y_train)
</code></pre>
<h4 id="梯度提升"><a href="#梯度提升" class="headerlink" title="梯度提升"></a>梯度提升</h4><p>另一个非常著名的提升算法是梯度提升<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>与 Adaboost 一样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>梯度提升也是通过向集成中逐步增加分类器运行的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>每一个分类器都修正之前的分类结果<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>然而<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它并不像 Adaboost 那样每一次迭代都更改实例的权重<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这个方法是去使用新的分类器去拟合前面分类器预测的 <em>残差</em> <span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><code>GradientBoostingRegressor</code>也支持指定用于训练每棵树的训练实例比例的超参数<code>subsample</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>例如如果<code>subsample=0.25</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>那么每个树都会在 25% 随机选择的训练实例上训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>你现在也能猜出</p>
<h3 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h3><p>本章讨论的最后一个集成方法叫做 <em>Stacking</em><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><em>stacked generalization</em> 的缩写<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这个算法基于一个简单的想法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>不使用琐碎的函数<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>如硬投票<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>来聚合集合中所有分类器的预测<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们为什么不训练一个模型来执行这个聚合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>？</h-inner></h-char></span>图 7-12 展示了这样一个在新的回归实例上预测的集成<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>底部三个分类器每一个都有不同的值<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>3.1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>2.7 和 2.9<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后最后一个分类器<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>叫做 <em>blender</em> 或者_元学习器_<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>把这三个分类器的结果当做输入然后做出最终决策<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>3.0<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span><br><img src="/img/loading.gif" data-original="/img/7-12.png"></p>
<h3 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h3><p><img src="/img/loading.gif" data-original="/images/Pasted%20image%2020230216155927.png"></p>
<p><img src="/img/loading.gif" data-original="/images/Pasted%20image%2020230216155852.png"></p>
<p><img src="/img/loading.gif" data-original="/images/Pasted%20image%2020230216155909.png"></p>
<h2 id="更快的优化器"><a href="#更快的优化器" class="headerlink" title="更快的优化器"></a>更快的优化器</h2><p><img src="/img/loading.gif" data-original="/images/Pasted%20image%2020230216155719.png"></p>
<p>训练一个非常大的深度神经网络可能会非常缓慢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 到目前为止<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们已经看到了四种加速训练的方法<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>并且达到更好性能的方法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><strong>对连接权重应用良好的初始化策略<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使用良好的激活函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使用批归一化以及重用预训练网络的部分</strong> <span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>使用辅助任务或无监督学习<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 另一个速度提升的方法是使用<strong>更快的优化器</strong> <span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而不是常规的梯度下降优化器<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>动量优化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>Nesterov 加速梯度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>AdaGrad<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>RMSProp<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>最后是 Adam 和 Nadam 优化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>回想一下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>梯度下降只是通过直接减去损失函数<code>J(θ)</code>相对于权重<code>θ</code>的梯度<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><code>∇θJ(θ)</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>乘以学习率<code>η</code>来更新权重<code>θ</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 等式是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>θ ← θ – η ∇[θ]J(θ)</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>它不关心早期的梯度是什么<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 如果局部梯度很小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>则会非常缓慢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>动量优化很关心以前的梯度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>在每次迭代时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它将动量向量<code>m</code><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>乘以学习率<code>η</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>与局部梯度相加<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并且通过简单地减去该动量向量来更新权重<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>参见公式 11-4<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 换句话说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>梯度用作加速度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不用作速度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 为了模拟某种摩擦机制<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>避免动量过大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>该算法引入了一个新的超参数<code>β</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>简称为动量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它必须设置在 0<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>高摩擦<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>和 1<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>无摩擦<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>之间<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 典型的动量值是 0.9<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br><img src="/img/loading.gif" data-original="/img/f8b8fdfbaf932d63888e504c03fef03b.png"></p>
<p>可以很容易验证<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果梯度保持不变<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>则最终速度<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>即<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>权重更新的最大大小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>等于该梯度乘以学习率<code>η</code>乘以<code>1/(1-β)</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果<code>β = 0.9</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>则最终速度等于学习率的梯度乘以 10 倍<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因此动量优化比梯度下降快 10 倍<span class="bd-box"><h-char class="bd bd-beg"><h-inner>！</h-inner></h-char></span> 这使动量优化比梯度下降快得多<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 特别是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们在第四章中看到<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>当输入量具有非常不同的尺度时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>损失函数看起来像一个细长的碗<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>见图 4-7<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 梯度下降速度很快<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但要花很长的时间才能到达底部<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> <em>相反<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>动量优化会越来越快地滚下山谷底部<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>直到达到底部<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>最佳<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>在不使用批归一化的深度神经网络中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>较高层往往会得到具有不同的尺度的输入<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以使用动量优化会有很大的帮助</em> <span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 它也可以帮助滚过局部最优值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>Yurii Nesterov 在 1983 年提出的动量优化的一个小变体几乎总是比普通的动量优化更快<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> Nesterov 动量优化或 Nesterov 加速梯度<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>Nesterov Accelerated Gradient<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>NAG<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>的思想是测量损失函数的梯度不是在局部位置<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而是在动量方向稍微靠前<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>见公式 11-5<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 与普通的动量优化的唯一区别在于梯度是在<code>θ+βm</code>而不是在<code>θ</code>处测量的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/d94e1afbe24244563a6b170bfbba856f.png"></p>
<p>再次考虑细长碗的问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><strong>梯度下降从最陡峭的斜坡快速下降<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后缓慢地下到谷底<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 如果算法能够早期检测到这个问题并且纠正它的方向来指向全局最优点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>那将是非常好的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>AdaGrad 算法通过沿着最陡的维度缩小梯度向量来实现这一点<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>见公式 11-6<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></strong> </p>
<p><img src="/img/loading.gif" data-original="/img/ecc26257570ef444e6a1ce1029e7f307.png"></p>
<p>第一步将梯度的平方累加到向量<code>s</code>中<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>⊗符号表示元素级别相乘<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 这个向量化形式相当于向量<code>s</code>的每个元素<code>s[i]</code>计算<code>s[i] ← s[i] + (∂J(θ)/∂θ[i])^2</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>换一种说法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>每个<code>s[i]</code>累加损失函数对参数<code>θ[i]</code>的偏导数的平方<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 如果损失函数沿着第<code>i</code>维陡峭<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>则在每次迭代时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><code>s[i]</code>将变得越来越大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>第二步几乎与梯度下降相同<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但有一个很大的不同<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>梯度向量按比例<code>(s+ε)^0.5</code>缩小 <span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><code>⊘</code>符号表示元素分割<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><code>ε</code>是避免被零除的平滑项<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>通常设置为<code>10^(-10)</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 这个向量化的形式相当于所有<code>θ[i]</code>同时计算</p>
<p><img src="/img/loading.gif" data-original="/img/1d42dc52ad9a49a5ba54ef885454e778.png"></p>
<p><strong>前面看到<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>AdaGrad 的风险是降速太快<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可能无法收敛到全局最优<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>RMSProp 算法通过仅累积最近迭代<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>而不是从训练开始以来的所有梯度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>的梯度来修正这个问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></strong> 它通过在第一步中使用指数衰减来实现<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>见公式 11-7<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/731f1ffe649c5a195ed46439968d7a21.png"></p>
<p>如果你只看步骤 1, 2 和 5<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你会注意到 Adam 与动量优化和 RMSProp 的相似性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 唯一的区别是第 1 步计算指数衰减的平均值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而不是指数衰减的和<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但除了一个常数因子<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>衰减平均值只是衰减和的<code>1 - β1</code>倍<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>之外<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它们实际上是等效的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>动量衰减超参数<code>β1</code>通常初始化为 0.9<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而缩放衰减超参数<code>β2</code>通常初始化为 0.999<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 如前所述<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>平滑项<code>ε</code>通常被初始化为一个很小的数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>例如<code>10^(-7)</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>实际上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>由于 Adam 是一种自适应学习率算法<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>如 AdaGrad 和 RMSProp<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以对学习率超参数<code>η</code>的调整较少<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 您经常可以使用默认值<code>η= 0.001</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使 Adam 相对于梯度下降更容易使用<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>表 11-2 比较了讨论过的优化器<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><code>*</code>是差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><code>**</code>是平均<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><code>***</code>是好<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/2d74b4c2999d3fa82e6b5aa85be19c1b.png"></p>
<p><img src="/img/loading.gif" data-original="/images/Pasted%20image%2020230216162030.png"></p>
<h2 id="正则化Regularization"><a href="#正则化Regularization" class="headerlink" title="正则化Regularization"></a>正则化Regularization</h2><p>有四个参数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我可以拟合一个大象<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>五个我可以让他摆动他的象鼻<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>—— John von Neumann,cited by Enrico Fermi in Nature 427</p>
<p>有数千个参数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>甚至可以拟合整个动物园<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>深度神经网络通常具有数以万计的参数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>有时甚至是数百万<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 有了这么多的参数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>网络拥有难以置信的自由度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以适应各种复杂的数据集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 但是这个很大的灵活性也意味着它很容易过拟合训练集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>所以需要正则<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>第 10 章用过了最好的正则方法之一<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>早停<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><a href="_posts/diary/2023-02-09.md#_EarlyStopping">_EarlyStopping</a></p>
<p>这一节会介绍其它一些最流行的神经网络正则化技术<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>ℓ1 和 ℓ2 正则<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>丢弃和最大范数正则<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>丢弃是深度神经网络最流行的正则化方法之一<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 它由 Geoffrey Hinton 于 2012 年提出<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并在 Nitish Srivastava 等人的 2014 年论文中进一步详细描述<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并且已被证明是非常成功的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>即使是最先进的神经网络<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>仅仅通过增加丢弃就可以提高 1-2% 的准确度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>这是一个相当简单的算法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><strong>在每个训练步骤中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>每个神经元<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>包括输入神经元<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但不包括输出神经元<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>都有一个暂时<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>丢弃<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char></span>的概率<code>p</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这意味着在这个训练步骤中它将被完全忽略<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> 在下一步可能会激活<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>见图 11-9<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 超参数<code>p</code>称为丢弃率<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>通常设为 10% 到 50% 之间<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>循环神经网络之间接近 20-30%<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在卷积网络中接近 40-50%<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 训练后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>神经元不会再丢失<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></strong> </p>
<p><img src="/img/loading.gif" data-original="/img/cc9f24739534a97e494d6cb6522e0f75.png"></p>
<blockquote>
<p>这个具有破坏性的方法竟然行得通<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这是相当令人惊讶的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>如果一个公司的员工每天早上被告知要掷硬币来决定是否上班<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>公司的表现会不会更好呢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>？</h-inner></h-char></span>那么<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>谁知道<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>也许会<span class="bd-box"><h-char class="bd bd-beg"><h-inner>！</h-inner></h-char></span>公司显然将被迫适应这样的组织构架<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>它不能依靠任何一个人操作咖啡机或执行任何其他关键任务<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以这个专业知识将不得不分散在几个人身上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>员工必须学会与其他的许多同事合作<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而不仅仅是其中的一小部分<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>该公司将变得更有弹性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>如果一个人离开了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并没有什么区别<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>目前还不清楚这个想法是否真的可以在公司实行<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但它确实对于神经网络是可行的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>神经元被丢弃训练不能与其相邻的神经元共适应<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>他们必须尽可能让自己变得有用<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>他们也不能过分依赖一些输入神经元;他们必须注意他们的每个输入神经元<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>他们最终对输入的微小变化会不太敏感<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>最后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你会得到一个更稳定的网络<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>泛化能力更强<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</blockquote>
<p><img src="/img/loading.gif" data-original="/images/Pasted%20image%2020230216165658.png"></p>
<h2 id="CNN-CV"><a href="#CNN-CV" class="headerlink" title="CNN-CV"></a>CNN-CV</h2><p>卷积神经网络<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>CNN<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>起源于人们对大脑视神经的研究<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>自从 1980 年代<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>CNN 就被用于图像识别了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>最近几年<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>得益于算力提高<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>训练数据大增<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>以及第 11 章中介绍过的训练深度网络的技巧<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>CNN 在一些非常复杂的视觉任务上取得了超出人类表现的进步<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>CNN 支撑了图片搜索<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>无人驾驶汽车<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>自动视频分类<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>等等<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>另外<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>CNN 也不再限于视觉<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>比如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>语音识别和自然语言处理<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但这一章只介绍视觉应用<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>David H. Hubel 和 Torsten Wiesel 在 1958 年和 1959 年在猫的身上做了一系列研究<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>对视神经中枢做了研究<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>并在 1981 年荣获了诺贝尔生理学或医学奖<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>特别的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>他们指出视神经中的许多神经元都有一个局部感受域<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>local receptive field<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>也就是说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这些神经元只对有限视觉区域的刺激作反应</p>
<h4 id="卷积层ConvalutionalLayer"><a href="#卷积层ConvalutionalLayer" class="headerlink" title="卷积层ConvalutionalLayer"></a>卷积层ConvalutionalLayer</h4><p>卷积层是 CNN 最重要的组成部分<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>第一个卷积层的神经元<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不是与图片中的每个像素点都连接<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而是只连着局部感受野的像素<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>见图 14-2<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>同理<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>第二个卷积层中的每个神经元也只是连着第一层中一个小方形内的神经元<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这种架构可以让第一个隐藏层聚焦于小的低级特征<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后在下一层组成大而高级的特征</p>
<p><img src="/img/loading.gif" data-original="/img/742bbcc20165cd543798934f27e16a25.png"></p>
<p>神经元的权重可以表示为感受野大小的图片<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>图 14-5 展示了两套可能的权重<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>称为权重<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>或卷积核<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>第一个是黑色的方形<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>中央有垂直白线<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><code>7 × 7</code>的矩阵<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>除了中间的竖线都是 1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其它地方是 0<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>使用这个矩阵<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>神经元只能注意到中间的垂直线<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>因为其它地方都乘以 0 了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>第二个过滤器也是黑色的方形<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是中间是水平的白线<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>使用这个权重的神经元只会注意中间的白色水平线<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>如果卷积层的所有神经元使用同样的垂直过滤器<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>和同样的偏置项<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>给神经网络输入图 14-5 中最底下的图片<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>卷积层输出的是左上的图片<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>可以看到<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>图中垂直的白线得到了加强<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其余部分变模糊了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>相似的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>右上的图是所有神经元都是用水平线过滤器的结果<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>水平的白线加强了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其余模糊了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>因此<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一层的全部神经元都用一个过滤器<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>就能输出一个特征映射<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>feature map<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>特征映射可以高亮图片中最为激活过滤器的区域<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>当然<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不用手动定义过滤器<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>卷积层在训练中可以自动学习对任务最有用的过滤器<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>上面的层则可以将简单图案组合为复杂图案<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br><img src="/img/loading.gif" data-original="/img/9e4747d2c0ff968eaa1b365b686a336b.png"></p>
<p>简单起见<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>前面都是将每个卷积层的输出用 2D 层来表示的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但真实的卷积层可能有多个过滤器<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>过滤器数量由你确定<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>每个过滤器会输出一个特征映射<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以表示成 3D 更准确</p>
<p><img src="/img/loading.gif" data-original="/img/e3e8c55fa98dd5af8dbe24fcd08d7dca.png"></p>
<pre><code class="python">下面代码使用 Scikit-Learn 的`load_sample_image()`加载了两张图片&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;，&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;一张是中国的寺庙&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;，&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;另一张是花&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;，&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;创建了两个过滤器&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;，&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;应用到了两张图片上&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;，&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;最后展示了一张特征映射&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;：&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;

from sklearn.datasets import load_sample_image

# 加载样本图片
china = load_sample_image(&quot;china.jpg&quot;) / 255
flower = load_sample_image(&quot;flower.jpg&quot;) / 255
/images = np.array([china, flower])
batch_size, height, width, channels = /images.shape

# 创建两个过滤器
filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)
filters[:, 3, :, 0] = 1  # 垂直线
filters[3, :, :, 1] = 1  # 水平线

outputs = tf.nn.conv2d(/images, filters, strides=1, padding=&quot;same&quot;)

plt.imshow(outputs[0, :, :, 1], cmap=&quot;gray&quot;) # 画出第 1 张图的第 2 个特征映射
plt.show() 

这个例子中&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;，&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;我们手动定义了过滤器&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;，&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;但在真正的 CNN 中&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;，&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;一般将过滤器定义为可以训练的变量&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;，&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;好让神经网络学习哪个过滤器的效果最好&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;。&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;使用`keras.layers.Conv2D`层&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;：&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;

conv = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1,
                           padding=&quot;same&quot;, activation=&quot;relu&quot;) 
</code></pre>
<p><code>tf.nn.conv2d()</code>函数这一行<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>再多说说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<ul>
<li><p><code>/images</code>是一个输入的小批次<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>4D 张量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</li>
<li><p><code>filters</code>是过滤器的集合<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>也是 4D 张量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</li>
<li><p><code>strides</code>等于 1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>也可以是包含 4 个元素的 1D 数组<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>中间的两个元素是垂直和水平步长<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><code>s[h]</code>和<code>s[w]</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>第一个和最后一个元素现在必须是 1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>以后可以用来指定批次步长<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>跳过实例<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>和通道步长<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>跳过前一层的特征映射或通道<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</li>
<li><p><code>padding</code>必须是<code>&quot;same&quot;</code>或<code>&quot;valid&quot;</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
</li>
<li><p>如果设为<code>&quot;same&quot;</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>卷积层会使用零填充<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>输出的大小是输入神经元的数量除以步长<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>再取整<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>如果输入大小是 13<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>步长是 5<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>见图 14-7<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>则输出大小是 3<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><code>13 / 5 = 2.6</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>再向上圆整为 3<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>零填充尽量在输入上平均添加<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>当<code>strides=1</code>时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>层的输出会和输入有相同的空间维度<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>宽和高<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这就是<code>same</code>的来历<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</li>
<li><p>如果设为<code>&quot;valid&quot;</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>卷积层就不使用零填充<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>取决于步长<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可能会忽略图片的输入图片的底部或右侧的行和列<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>见图 14-7<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>简单举例<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>只是显示了水平维度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这意味着每个神经元的感受野位于严格确定的图片中的位置<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>不会越界<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这就是<code>valid</code>的来历<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</li>
</ul>
<blockquote>
<p>CNN 的另一个问题是卷积层需要很高的内存<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>特别是在训练时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因为反向传播需要所有前向传播的中间值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</blockquote>
<h4 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h4><p>明白卷积层的原理了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>池化层就容易多了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>池化层的目的是对输入图片做降采样<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>即<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>收缩<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>以降低计算负载<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>内存消耗和参数的数量<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>降低过拟合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>和卷积层一样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>池化层中的每个神经元也是之和前一层的感受野里的有限个神经元相连<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>和前面一样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>必须定义感受野的大小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>步长和填充类型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>但是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>池化神经元没有权重<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它所要做的是使用聚合函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>比如最大或平均<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>对输入做聚合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>图 14-8 展示了最为常用的最大池化层<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>在这个例子中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使用了一个<code>2 × 2</code>的池化核<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>步长为 2<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>没有填充<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>只有感受野中的最大值才能进入下一层<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其它的就丢弃了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br><img src="/img/loading.gif" data-original="/img/11350c8af71a04bfece796ad9c622220.png"></p>
<p>除了可以减少计算<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>内存消耗<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>参数数量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>最大池化层还可以带来对小偏移的不变性</p>
<p>在 CNN 中每隔几层就插入一个最大池化层<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以带来更大程度的平移不变性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>另外<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>最大池化层还能带来一定程度的旋转不变性和缩放不变性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>当预测不需要考虑平移<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>旋转和缩放时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>比如分类任务<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不变性可以有一定益处<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br><img src="/img/loading.gif" data-original="/img/5979b04b6b1410e6023c452c38561cdb.png"></p>
<p>要创建平均池化层<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>则使用<code>AvgPool2D</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>平均池化层和最大池化层很相似<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但计算的是感受野的平均值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>平均池化层在过去很流行<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但最近人们使用最大池化层更多<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因为最大池化层的效果更好<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>池化层还可以沿着深度方向做计算<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这可以让 CNN 学习到不同特征的不变性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>比如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>CNN 可以学习多个过滤器<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>每个过滤器检测一个相同的图案的不同旋转<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>比如手写字<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>见图 14-10<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>深度池化层可以使输出相同<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>CNN 还能学习其它的不变性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>厚度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>明亮度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>扭曲<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>颜色<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>等等<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br><img src="/img/loading.gif" data-original="/img/b36a6bb4d3a0783b1bc2bdcfe475e592.png"></p>
<h4 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h4><p><strong>数据增强是通过生成许多训练实例的真实变种<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>来人为增大训练集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>因为可以降低过拟合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>成为了一种正则化方法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></strong> 生成出来的实例越真实越好<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>最理想的情况<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>人们无法区分增强图片是原生的还是增强过的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>简单的添加白噪声没有用<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>增强修改要是可以学习的<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>白噪声不可学习<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>&#x3D;&#x3D;可以轻微偏移<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>旋转<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>缩放原生图<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>再添加到训练集中&#x3D;&#x3D;<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>见图 14-12<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这么做可以使模型对位置<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>方向和物体在图中的大小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>有更高的容忍度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>如果想让模型对不同光度有容忍度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以生成对比度不同的照片<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>通常<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>&#x3D;&#x3D;还可以水平翻转图片<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>文字不成<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>不对称物体也不成<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>&#x3D;&#x3D;<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>通过这些变换<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以极大的增大训练集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h4 id="CNN的典型架构"><a href="#CNN的典型架构" class="headerlink" title="CNN的典型架构"></a>CNN的典型架构</h4><p>CNN 的典型架构是将几个卷积层叠起来<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>每个卷积层后面跟着一个 ReLU 层<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后再叠一个池化层<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后再叠几个卷积层<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>+ReLU<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>接着再一个池化层<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><strong>以此类推<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>图片在流经神经网络的过程中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>变得越来越小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但得益于卷积层<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>却变得越来越深<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>特征映射变多了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span></strong> 见图 14-11<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>在 CNN 的顶部<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>还有一个常规的前馈神经网络<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>由几个全连接层<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>+ReLU<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>组成<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>最终层输出预测<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>比如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一个输出类型概率的 softmax 层<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br><img src="/img/loading.gif" data-original="/img/752d98a79871d001835f8948d8b75626.png"></p>
<pre><code class="python">model = keras.models.Sequential([
    keras.layers.Conv2D(64, 7, activation=&quot;relu&quot;, padding=&quot;same&quot;,
                        input_shape=[28, 28, 1]),
    keras.layers.MaxPooling2D(2),
    keras.layers.Conv2D(128, 3, activation=&quot;relu&quot;, padding=&quot;same&quot;),
    keras.layers.Conv2D(128, 3, activation=&quot;relu&quot;, padding=&quot;same&quot;),
    keras.layers.MaxPooling2D(2),
    keras.layers.Conv2D(256, 3, activation=&quot;relu&quot;, padding=&quot;same&quot;),
    keras.layers.Conv2D(256, 3, activation=&quot;relu&quot;, padding=&quot;same&quot;),
    keras.layers.MaxPooling2D(2),
    keras.layers.Flatten(),
    keras.layers.Dense(128, activation=&quot;relu&quot;),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(64, activation=&quot;relu&quot;),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(10, activation=&quot;softmax&quot;)
]) 
</code></pre>
<p>我们先看看经典的 LeNet-5 架构<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>1998<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后看看三个 ILSVRC 竞赛的冠军<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>AlexNet<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>2012<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>GoogLeNet<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>2014<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>ResNet<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>2015<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=https://homl.info/lenet5">LeNet-5</a> 也许是最广为人知的 CNN 架构<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>前面提到过<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它是由 Yann LeCun 在 1998 年创造出来的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>被广泛用于手写字识别<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>MNIST<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>它的结构如下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><br><img src="/img/loading.gif" data-original="/img/d042bd08d28fcf0eebb6b1d517815272.png"></p>
<p>AlexNet 和 LeNet-5 很相似<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>只是更大更深<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>是首个将卷积层堆叠起来的网络<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而不是在每个卷积层上再加一个池化层<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>为了降低过拟合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>作者使用了两种正则方法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>首先<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>F8 和 F9 层使用了丢弃<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>丢弃率为 50%<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>其次<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>他们通过随机距离偏移训练图片<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>水平翻转<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>改变亮度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>做了数据增强<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br><img src="/img/loading.gif" data-original="/img/61021035567a4df1ec049f0fdf9bcc21.png"></p>
<p><a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=https://homl.info/81">GoogLeNet 架构</a>能取得这么大的进步<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>很大的原因是它的网络比之前的 CNN 更深<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>见图 14-14<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这归功于被称为创始模块<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>inception module<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>的子网络<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它可以让 GoogLeNet 可以用更高的效率使用参数<br><img src="/img/loading.gif" data-original="/img/2262948e82010dbec63937ed0ca4b096.png"></p>
<p>ResNet 的使用了极深的卷积网络<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>共 152 层<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>其它的变体有 1450 或 152 层<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>反映了一个总体趋势<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>模型变得越来越深<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>参数越来越少<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>训练这样的深度网络的方法是使用跳连接<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>也被称为快捷连接<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>输入信号添加到更高层的输出上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>训练神经网络时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>目标是使网络可以对目标函数<code>h(x)</code>建模<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>如果将输入<code>x</code>添加给网络的输出<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>即<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>添加一个跳连接<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>则网络就要对<code>f(x) = h(x) – x</code>建模<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而不是<code>h(x)</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这被称为残差学习<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>见图 14-15<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="/img/546be8127641972a5bb9b137d8c0e98e.png"></p>
<p><img src="/img/loading.gif" data-original="/img/50c635c122c979e434cd2ac2b7bd2fea.png"></p>
<h4 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h4><p>分类并定位图片中的多个物体的任务被称为目标检测<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>几年之前<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使用的方法还是用定位单一目标的 CNN<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后将其在图片上滑动</p>
<p>用这个简单的方法来做目标检测的效果相当不错<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但需要运行 CNN 好几次<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以很慢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>幸好<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>有一个更快的方法来滑动 CNN<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>使用全卷积网络<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>fully convolutional network<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>FCN<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>YOLO 是一个非常快且准确的目标检测框架<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>是 Joseph Redmon 在 2015 年的一篇<a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=https://homl.info/yolo">论文</a>中提出的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>2016 年优化为 <a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=https://homl.info/yolo2">YOLOv2</a><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>2018 年优化为 <a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=https://homl.info/yolo3">YOLOv3</a><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>速度快到甚至可以在实时视频中运行</p>
<h4 id="语义分割"><a href="#语义分割" class="headerlink" title="语义分割"></a>语义分割</h4><p>在语义分割中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>每个像素根据其所属的目标来进行分类<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>路<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>汽车<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>行人<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>建筑物<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>等等<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>见图 14-26<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>注意<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>相同类的不同目标是不做区分的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>分割图片的右侧的所有自行车被归类为一坨像素<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这个任务的难点是当图片经过常规 CNN 时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>会逐渐丢失空间分辨率<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>因为有的层的步长大于 1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>因此<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>常规的 CNN 可以检测出图片的左下有一个人<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但不知道准确的位置<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br><img src="/img/loading.gif" data-original="/img/c15e385180dd286f3ba25a73a9dc40cf.png"></p>
<h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><p>RNN 不是唯一能处理序列数据的神经网络<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>对于小序列<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>常规紧密网络也可以<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>对于长序列<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>比如音频或文本<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>卷积神经网络也可以<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>我们会讨论这两种方法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>本章最后会实现一个 WaveNet<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>这是一种 CNN 架构<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以处理上万个时间步的序列<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>在第 16 章<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>还会继续学习 RNN<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如何使用 RNN 来做自然语言处理<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>和基于注意力机制的新架构<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>我们主要关注的是前馈神经网络<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>激活仅从输入层到输出层的一个方向流动<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>附录 E 中的几个网络除外<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 循环神经网络看起来非常像一个前馈神经网络<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>除了它也有连接指向后方<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 让我们看一下最简单的 RNN<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>由一个神经元接收输入<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>产生一个输出<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并将输出发送回自己<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如图 15-1<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>左<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>所示<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="img/0e364b0ec66b501a0e4b3ecc75fadeb5.png"></p>
<p>每个循环神经元有两组权重<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>一组用于输入<code>x[t]</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>另一组用于前一时间步长<code>y[t - 1]</code>的输出<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 我们称这些权重向量为<code>w[x]</code>和<code>w[y]</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>如果考虑的是整个循环神经元层<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以将所有权重向量放到两个权重矩阵中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><code>W[x]</code>和<code>W[y]</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>整个循环神经元层的输出可以用公式 15-1 表示<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><code>b</code>是偏差项<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><code>φ(·)</code>是激活函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>例如 ReLU<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="img/7d054b04b86f7184d742ce4fd79ae23e.png"></p>
<p>一般情况下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>时间步<code>t</code>的单元状态<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>记为<code>h[t]</code><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><code>h</code>代表<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>隐藏<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>是该时间步的某些输入和前一时间步状态的函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>h[t] = f(h[t - 1], x[t])</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 其在时间步<code>t</code>的输出<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>表示为<code>y[t]</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>也和前一状态和当前输入的函数有关<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br><img src="/img/loading.gif" data-original="img/7412ce440bf5c79fe186f415e7206c4b.png"></p>
<p>RNN 可以同时输入序列并输出序列<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>见图 15-4<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>左上角的网络<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这种序列到序列的网络可以有效预测时间序列<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>如股票价格<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>输入过去<code>N</code>天价格<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>则输出向未来移动一天的价格<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>即<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>从<code>N - 1</code>天前到明天<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>或者<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你可以向网络输入一个序列<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>忽略除最后一项之外的所有输出<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>图 15-4 右上角的网络<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 换句话说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这是一个序列到向量的网络<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>你可以向网络输入与电影评论相对应的单词序列<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>网络输出情感评分<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>从<code>-1 [讨厌]</code>到<code>+1 [喜欢]</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>相反<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以向网络一遍又一遍输入相同的向量<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>见图 15-4 的左下角<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>输出一个序列<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这是一个向量到序列的网络<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>输入可以是图像<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>或是 CNN 的结果<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>输出是该图像的标题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>给网络输入一种语言的一句话<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>编码器会把这个句子转换成单一的向量表征<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后解码器将这个向量解码成另一种语言的句子<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 这种称为编码器 - 解码器的两步模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>比用单个序列到序列的 RNN 实时地进行翻译要好得多<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因为句子的最后一个单词可以影响翻译的第一句话<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以你需要等到听完整个句子才能翻译<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>第 16 章还会介绍如何实现编码器-解码器<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>会比图 15-4 中复杂<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="img/071528c1638f48307509ce23a53f8431.png"></p>
<p>训练 RNN 诀窍是在时间上展开<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>就像我们刚刚做的那样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后只要使用常规反向传播<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>见图 15-5<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 这个策略被称为时间上的反向传播<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>BPTT<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="img/2eb72b6016c50e7bab66a67a1530df86.png"></p>
<p>假设你在研究网站每小时的活跃用户数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>或是所在城市的每日气温<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>或公司的财务状况<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>用多种指标做季度衡量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>在这些任务中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>数据都是一个序列<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>每步有一个或多个值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这被称为时间序列<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br><img src="/img/loading.gif" data-original="img/cdee2dcc8d620310d86fa491e3ea8ffa.png"></p>
<p>使用 RNN 之前<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>最好有基线指标<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>否则做出来的模型可能比基线模型还糟<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>最简单的方法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>是预测每个序列的最后一个值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这个方法被称为朴素预测<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>有时很难被超越<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>在这个例子中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它的均方误差为 0.020<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p>另一个简单的方法是使用全连接网络<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>因为结果要是打平的特征列表<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>需要加一个<code>Flatten</code>层<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>使用简单线性回归模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使预测值是时间序列中每个值的线性组合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<pre><code class="python">&gt;&gt;&gt; y_pred = X_valid[:, -1]
&gt;&gt;&gt; np.mean(keras.losses.mean_squared_error(y_valid, y_pred))
0.020211367 

model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[50, 1]),
    keras.layers.Dense(1)
]) 

model = keras.models.Sequential([
  keras.layers.SimpleRNN(1, input_shape=[None, 1])
]) 
</code></pre>
<p>将多个神经元的层堆起来<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>见图 15-7<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>就形成了深度 RNN<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="/img/loading.gif" data-original="img/c31153ab45ed5520564b4fc8d2c267a5.png"></p>
<p>在训练长序列的 RNN 模型时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>必须运行许多时间步<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>展开的 RNN 变成了一个很深的网络<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>正如任何深度神经网络一样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它面临不稳定梯度问题<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>第 11 章讨论过<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使训练无法停止<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>或训练不稳定<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>另外<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>当 RNN 处理长序列时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>RNN 会逐渐忘掉序列的第一个输入<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>下面就来看看这两个问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>先是第一个问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>很多之前讨论过的缓解不稳定梯度的技巧都可以应用在 RNN 中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>好的参数初始化方式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>更快的优化器<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>丢弃<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>等等<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>但是非饱和激活函数<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>如 ReLU<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>的帮助不大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>事实上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它会导致 RNN 更加不稳定<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>为什么呢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>？</h-inner></h-char></span>假设梯度下降更新了权重<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以令第一个时间步的输出提高<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>因为每个时间步使用的权重相同<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>第二个时间步的输出也会提高<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这样就会导致输出爆炸 —— 不饱和激活函数不能阻止这个问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>要降低爆炸风险<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以使用更小的学习率<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>更简单的方法是使用一个饱和激活函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>比如双曲正切函数<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>这就解释了为什么 tanh 是默认选项<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>另外<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>批归一化也没什么帮助<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>事实上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不能在时间步骤之间使用批归一化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>只能在循环层之间使用<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>更加准确点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>技术上可以将 BN 层添加到记忆单元上<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>后面会看到<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这样就可以应用在每个时间步上了<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>既对输入使用<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>也对前一步的隐藏态使用<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br>使用<code>tf.keras</code>在一个简单记忆单元中实现层归一化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br>另一种归一化的形式效果好些<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>层归一化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>它是由 Jimmy Lei Ba 等人在 2016 年的<a target="_blank" rel="noopener" href="https://links.jianshu.com/go?to=https://homl.info/layernorm">一篇论文</a>中提出的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>它跟批归一化很像<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但不是在批次维度上做归一化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而是在特征维度上归一化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这么做的一个优势是可以独立对每个实例<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>实时计算所需的统计量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这还意味着训练和测试中的行为是一致的<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>这点和 BN 相反<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>且不需要使用指数移动平均来估计训练集中所有实例的特征统计<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>由于数据在 RNN 中流动时会经历转换<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>每个时间步都损失了一定信息<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>一定时间后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>第一个输入实际上会在 RNN 的状态中消失<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>

      
    </div>
    <footer class="article-footer">
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../../way2/archives/" rel="tag">archives</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../../way2/ml/" rel="tag">ml</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="../../../../03/15/notes/C++%20Primer%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          C++ Primer 学习笔记
        
      </div>
    </a>
  
  
    <a href="../../../20/notes/CLRSReview/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">
        
          2022Fall_算法导论笔记
        
      </div>
    </a>
  
</nav>

  
</article>


</section>
        
      </div>
      <footer id="footer">
  
    <aside id="sidebar" class="outer">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="../../../../../2025/05/04/notes/%E4%BB%8E%20LAMP%20%E5%88%B0%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB%EF%BC%9A%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AE%9E%E8%B7%B5/">从 LAMP 到前后端分离：架构演进与技术实践</a>
          </li>
        
          <li>
            <a href="../../../../../2025/02/18/posts/Windows%E9%83%A8%E7%BD%B2Hexon%E5%8F%AF%E8%A7%86%E5%8C%96%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83/">Windows部署Hexon可视化博客环境</a>
          </li>
        
          <li>
            <a href="../../../../../2024/12/08/notes/Pinctrl%E5%92%8CGPIO%E5%AD%90%E7%B3%BB%E7%BB%9F/">Pinctrl和GPIO子系统</a>
          </li>
        
          <li>
            <a href="../../../../../2024/11/29/notes/%E9%A9%B1%E5%8A%A8%E8%BF%9B%E5%8C%96%E4%B9%8B%E8%B7%AF/">驱动进化之路</a>
          </li>
        
          <li>
            <a href="../../../../../2024/11/28/notes/%E5%B8%B8%E7%94%A8%E4%BE%9D%E8%B5%96%E5%8C%85%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7%E6%8D%A2%E6%BA%90%E6%96%B9%E6%B3%95%E5%A4%A7%E5%85%A8/">常用依赖包管理工具换源方法大全</a>
          </li>
        
          <li>
            <a href="../../../../../2024/11/28/projs/ReelBlend/">ReelBlend：一键生成扫街照片与 POV 视频</a>
          </li>
        
          <li>
            <a href="../../../../../2024/11/19/notes/Linux%20I2C%20%E5%BA%94%E7%94%A8%E7%BC%96%E7%A8%8B/">I2C应用编程</a>
          </li>
        
          <li>
            <a href="../../../../../2024/09/03/notes/C++%20%E9%9D%A2%E7%BB%8F%E4%BB%A5%E5%8F%8A%E7%AD%94%E6%A1%88/">C++ 面经以及答案</a>
          </li>
        
          <li>
            <a href="../../../../09/03/notes/%E6%B1%9F%E7%A7%91%E5%A4%A7STM32%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">江科大STM32学习笔记</a>
          </li>
        
          <li>
            <a href="../../../../07/05/notes/23Spring_%E5%BB%BA%E6%A8%A1%E5%A4%8D%E4%B9%A0/">23Spring_建模复习</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="../../../../../way2/algorithm/" style="font-size: 10px;">algorithm</a> <a href="../../../../../way2/archives/" style="font-size: 20px;">archives</a> <a href="../../../../../way2/cpp/" style="font-size: 16.67px;">cpp</a> <a href="../../../../../way2/diary/" style="font-size: 10px;">diary</a> <a href="../../../../../way2/embed/" style="font-size: 10px;">embed</a> <a href="../../../../../way2/linux/" style="font-size: 13.33px;">linux</a> <a href="../../../../../way2/ml/" style="font-size: 10px;">ml</a> <a href="../../../../../way2/note/" style="font-size: 10px;">note</a> <a href="../../../../../way2/project/" style="font-size: 10px;">project</a> <a href="../../../../../way2/share/" style="font-size: 13.33px;">share</a>
    </div>
  </div>

  
</aside>
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
        <div id="footer-info" class="inner" style="border-top:1px solid \#e0e0e0;padding-top:1.5rem;margin-top:2rem;color:#666">© 2025 <a href="https://github.com/kpkpy" target="_blank">kpkpy</a><br>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a><br></div>
      
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="../../../../../index.html" class="mobile-nav-link">Home</a>
  
    <a href="../../../../../way2/archives" class="mobile-nav-link">Archives</a>
  
    <a href="../../../../../about" class="mobile-nav-link">About</a>
  
</nav>
    

<script src="../../../../../js/clipboard.min.js"></script>
<script src="../../../../../js/jquery-1.4.3.min.js"></script>

<script src="../../../../../fancybox/jquery.fancybox-1.3.4.pack.js"></script>


<script src="../../../../../js/script.js"></script>






<script>
  MathJax = {
    options: {
      enableMenu: false
    },
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
    }
  };
</script>
<!-- <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    CommonHTML: {
      linebreaks: false
    }
  });
  </script> -->
<script type="text/javascript" id="MathJax-script" async
  src="../../../../../mathjax/tex-chtml.js">
</script>
<!-- <script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_CHTML">
</script> -->



  </div>

        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a,i=c[o];e=function(){c=c.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(n=new Image,a=t.getAttribute("data-original"),n.onload=function(){t.src=a,t.removeAttribute("data-original"),e&&e()},t.src!==a&&(n.src=a))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this);</script></body>
</html>